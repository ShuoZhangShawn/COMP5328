{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to run\n",
    "There are three section in this notebook. \n",
    "The first one is transition matrix estimation. The second one relates to CNN and the last one relates to MLP.\n",
    "\n",
    "### Installation\n",
    "<code>pip install -r requirements.txt</code>\n",
    "\n",
    "### Usage\n",
    "1. copy the dataset file under **/data** folder\n",
    "2. unzip the dataset zip file\n",
    "3. the structure of **/data** folder should be \n",
    "\n",
    "<code>\n",
    "data/\n",
    "└── 2024_A2_datasets/\n",
    "    ├── CIFAR10.npz\n",
    "    ├── FashionMNIST0.3.npz\n",
    "    └── FashionMNIST0.6.npz\n",
    "</code>\n",
    "\n",
    "4. Run /Assignment2_final.ipynb\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Transition matrix estimation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T10:15:58.513367Z",
     "start_time": "2024-11-04T10:15:57.309855Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.est.load import load_data\n",
    "import numpy as np\n",
    "from utils.est.simpleCNN import simple_cnn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "from utils.est.modeltrain import train\n",
    "from utils.est.modeltest import test\n",
    "from utils.est.help import seed_torch, accuracy, AverageMeter, load_model\n",
    "from utils.est.Dataloader import get_loader"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T10:16:01.088707Z",
     "start_time": "2024-11-04T10:16:00.818648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cifar10X_tr, cifar10y_tr,cifar10X_ts,cifar10y_ts = load_data(\"data/2024_A2_datasets/CIFAR10.npz\")\n",
    "cifar10X_tr.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 3, 32, 32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T10:16:06.892517Z",
     "start_time": "2024-11-04T10:16:05.931775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader, test_loader = get_loader(data=(cifar10X_tr, cifar10y_tr, cifar10X_ts, cifar10y_ts))\n",
    "model = simple_cnn(in_channels=3, num_classes=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T10:57:51.302131Z",
     "start_time": "2024-11-03T10:53:01.502076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# define model, loss function, optimizer\n",
    "model = simple_cnn(in_channels=3, num_classes=4).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# set epoch\n",
    "num_epochs = 10\n",
    "\n",
    "# train and validation\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "    train(epoch, model, optimizer, criterion, train_loader)\n",
    "\n",
    "    val_accuracy = test(epoch, model, criterion, test_loader, is_test=False)\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "test_accuracy = test(num_epochs, model, criterion, test_loader, is_test=True)\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train epoch  1  Accuracy  52.7\n",
      "Validate epoch  1  Accuracy  67.425\n",
      "Epoch 2/10\n",
      "Train epoch  2  Accuracy  64.58\n",
      "Validate epoch  2  Accuracy  72.55\n",
      "Epoch 3/10\n",
      "Train epoch  3  Accuracy  67.29\n",
      "Validate epoch  3  Accuracy  76.3\n",
      "Epoch 4/10\n",
      "Train epoch  4  Accuracy  69.315\n",
      "Validate epoch  4  Accuracy  78.125\n",
      "Epoch 5/10\n",
      "Train epoch  5  Accuracy  71.26\n",
      "Validate epoch  5  Accuracy  80.6\n",
      "Epoch 6/10\n",
      "Train epoch  6  Accuracy  72.28\n",
      "Validate epoch  6  Accuracy  79.9\n",
      "Epoch 7/10\n",
      "Train epoch  7  Accuracy  74.04\n",
      "Validate epoch  7  Accuracy  80.725\n",
      "Epoch 8/10\n",
      "Train epoch  8  Accuracy  75.085\n",
      "Validate epoch  8  Accuracy  81.725\n",
      "Epoch 9/10\n",
      "Train epoch  9  Accuracy  76.465\n",
      "Validate epoch  9  Accuracy  82.15\n",
      "Epoch 10/10\n",
      "Train epoch  10  Accuracy  77.83\n",
      "Validate epoch  10  Accuracy  81.875\n",
      "Evaluating on test set...\n",
      "Test epoch  10  Accuracy  81.875\n",
      "Final Test Accuracy: 81.88%\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T10:57:51.357144Z",
     "start_time": "2024-11-03T10:57:51.324778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.est.estimate_transition_matrix import estimate_transition_matrix\n",
    "from utils.est.create_anchor_loader import create_anchor_loader"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T10:58:08.806984Z",
     "start_time": "2024-11-03T10:57:51.358356Z"
    }
   },
   "cell_type": "code",
   "source": [
    "anchor_loaders = create_anchor_loader(train_loader, model, threshold=0.6)\n",
    "\n",
    "transition_matrix = estimate_transition_matrix(model, anchor_loaders, num_classes=4)\n",
    "print(\"Estimated Transition Matrix:\")\n",
    "print(transition_matrix)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Transition Matrix:\n",
      "[[0.86536193 0.09298614 0.02862846 0.01302442]\n",
      " [0.02364569 0.88456053 0.09023315 0.00156059]\n",
      " [0.03252944 0.01227329 0.85053104 0.10466633]\n",
      " [0.1318707  0.00474575 0.07453028 0.78885317]]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T10:58:08.868286Z",
     "start_time": "2024-11-03T10:58:08.808522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "def preprocess_images(X):\n",
    "    X_resized = []\n",
    "    for img in X:\n",
    "        #from (1, 28, 28) to (28, 28)\n",
    "        img = img.squeeze(0)\n",
    "        \n",
    "        # grey to RGB (28, 28) -> (28, 28, 3)\n",
    "        img_rgb = np.stack([img] * 3, axis=-1) \n",
    "        \n",
    "        # PIL Image\n",
    "        img_rgb_pil = Image.fromarray(img_rgb.astype(np.uint8))\n",
    "        img_resized_pil = img_rgb_pil.resize((32, 32), Image.BILINEAR)  #resize\n",
    "        \n",
    "        img_resized = np.array(img_resized_pil)\n",
    "        \n",
    "        img_resized = np.transpose(img_resized, (2, 0, 1))  # 转换为 (3, 32, 32)\n",
    "        \n",
    "        X_resized.append(img_resized)\n",
    "    \n",
    "    return np.array(X_resized)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T10:58:08.912308Z",
     "start_time": "2024-11-03T10:58:08.870069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "f3X_tr, f3y_tr,f3X_ts,f3y_ts = load_data(\"data/2024_A2_datasets/FashionMNIST0.3.npz\")\n",
    "print(f3X_tr.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 1, 28, 28)\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T10:58:09.739981Z",
     "start_time": "2024-11-03T10:58:08.913542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "f3Xtr_resized = preprocess_images(f3X_tr)\n",
    "f3X_ts_resized = preprocess_images(f3X_ts)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T10:58:09.823419Z",
     "start_time": "2024-11-03T10:58:09.740862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader2, test_loader2 = get_loader(data=(f3Xtr_resized, f3y_tr, f3X_ts_resized, f3y_ts))\n",
    "model2 = simple_cnn(in_channels=3, num_classes=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:03:17.519629Z",
     "start_time": "2024-11-03T10:58:09.824211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# define model, loss function\n",
    "model2 = simple_cnn(in_channels=3, num_classes=4).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.001)\n",
    "\n",
    "# set epoch\n",
    "num_epochs = 10\n",
    "\n",
    "# train and validation\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "    train(epoch, model2, optimizer, criterion, train_loader2)\n",
    "\n",
    "    val_accuracy = test(epoch, model2, criterion, test_loader2, is_test=False)\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "test_accuracy = test(num_epochs, model2, criterion, test_loader2, is_test=True)\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train epoch  1  Accuracy  43.7125\n",
      "Validate epoch  1  Accuracy  62.05\n",
      "Epoch 2/10\n",
      "Train epoch  2  Accuracy  47.670833333333334\n",
      "Validate epoch  2  Accuracy  63.1\n",
      "Epoch 3/10\n",
      "Train epoch  3  Accuracy  48.425\n",
      "Validate epoch  3  Accuracy  63.375\n",
      "Epoch 4/10\n",
      "Train epoch  4  Accuracy  49.0875\n",
      "Validate epoch  4  Accuracy  63.05\n",
      "Epoch 5/10\n",
      "Train epoch  5  Accuracy  49.67916666666667\n",
      "Validate epoch  5  Accuracy  63.7\n",
      "Epoch 6/10\n",
      "Train epoch  6  Accuracy  50.09583333333333\n",
      "Validate epoch  6  Accuracy  62.275\n",
      "Epoch 7/10\n",
      "Train epoch  7  Accuracy  50.71666666666667\n",
      "Validate epoch  7  Accuracy  61.75\n",
      "Epoch 8/10\n",
      "Train epoch  8  Accuracy  51.67916666666667\n",
      "Validate epoch  8  Accuracy  62.3\n",
      "Epoch 9/10\n",
      "Train epoch  9  Accuracy  52.35\n",
      "Validate epoch  9  Accuracy  57.925\n",
      "Epoch 10/10\n",
      "Train epoch  10  Accuracy  53.90833333333333\n",
      "Validate epoch  10  Accuracy  60.075\n",
      "Evaluating on test set...\n",
      "Test epoch  10  Accuracy  60.075\n",
      "Final Test Accuracy: 60.08%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:03:17.528433Z",
     "start_time": "2024-11-03T11:03:17.522960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.est.estimate_transition_matrix import estimate_transition_matrix\n",
    "from utils.est.create_anchor_loader import create_anchor_loader"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:03:32.746743Z",
     "start_time": "2024-11-03T11:03:17.530543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "anchor_loaders = create_anchor_loader(train_loader2, model2, threshold=0.6)\n",
    "\n",
    "\n",
    "transition_matrix2 = estimate_transition_matrix(model2, anchor_loaders, num_classes=4)\n",
    "print(\"Estimated Transition Matrix:\")\n",
    "print(transition_matrix2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Transition Matrix:\n",
      "[[0.77741247 0.12019797 0.0281337  0.0742563 ]\n",
      " [0.02998707 0.74413919 0.21209586 0.01377792]\n",
      " [0.02160717 0.03702152 0.78858835 0.15278149]\n",
      " [0.11052448 0.01863763 0.05449284 0.81634617]]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:03:32.758186Z",
     "start_time": "2024-11-03T11:03:32.751260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the two matrices\n",
    "true_matrix = np.array([\n",
    "    [0.7, 0.3, 0, 0],\n",
    "    [0, 0.7, 0.3, 0],\n",
    "    [0, 0, 0.7, 0.3],\n",
    "    [0.3, 0, 0, 0.7]\n",
    "])\n",
    "\n",
    "estimated_matrix = np.array([\n",
    "    [0.77019763, 0.10626329, 0.03162862, 0.09191043],\n",
    "    [0.02642049, 0.75073814, 0.2105207,  0.0123205 ],\n",
    "    [0.01650259, 0.01779101, 0.81544185, 0.15026474],\n",
    "    [0.18038471, 0.01821926, 0.05068507, 0.75071126]\n",
    "])\n",
    "\n",
    "# Calculate Mean Squared Error and Mean Absolute Error\n",
    "mse = np.mean((true_matrix - estimated_matrix) ** 2)\n",
    "mae = np.mean(np.abs(true_matrix - estimated_matrix))\n",
    "\n",
    "mse, mae"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.007466059336063282), np.float64(0.069070838125))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Hyperparameter Optimization and Regularization on Fashion-MNIST Classification"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Check if MPS is available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data: (1, 28, 28)\n",
      "shape X_train torch.Size([16000, 3, 32, 32])\n",
      "shape X_test torch.Size([4000, 3, 32, 32])\n",
      "shape of train data: (3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "from utils.dataloader.FashionMNISTDataLoader import FashionMNISTDataLoader, CIFAR10DataLoader\n",
    "\n",
    "path = 'data/2024_A2_datasets/FashionMNIST0.6.npz'\n",
    "data_loader = FashionMNISTDataLoader(path=path, batch_size=64, sample_size=0.0001, train_percentage=0.8, device=device) \n",
    "train_loader, eval_loader, test_loader = data_loader.get_loaders()\n",
    "\n",
    "print('shape of train data:', data_loader.get_shape_of_sample())   \n",
    "\n",
    "path_cifar = 'data/2024_A2_datasets/CIFAR10.npz'\n",
    "data_loader_cifar = CIFAR10DataLoader(path=path_cifar, batch_size=64, sample_size=1, train_percentage=0.8, device=device)\n",
    "\n",
    "print('shape of train data:', data_loader_cifar.get_shape_of_sample())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-31 19:28:37,339] Using an existing study with name 'fashion_mnist_0_6_with_t_matrix' instead of creating a new one.\n",
      "/Users/niklas/Documents/UNI/Australien/Courses/MachineLearning/Lab1/5328_GroupProject/utils/models/hyperparametertuning.py:48: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
      "/Users/niklas/Documents/UNI/Australien/Courses/MachineLearning/Lab1/5328_GroupProject/utils/models/hyperparametertuning.py:49: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
      "/Users/niklas/Documents/UNI/Australien/Courses/MachineLearning/Lab1/5328_GroupProject/utils/models/hyperparametertuning.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_l1 = trial.suggest_loguniform('lambda_l1', 1e-5, 1e-2)\n",
      "[I 2024-10-31 19:31:58,929] Trial 1 finished with value: 0.37623282358156024 and parameters: {'lr': 0.0014847852286384534, 'dropout': 0.3024205954332167, 'kernel_size_conv1': 2, 'conv1_channels': 32, 'conv2_channels': 256, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 47, 'add_l1': True, 'criterion': 'nf_land_rce', 'lambda_l1': 3.9744845641176954e-05}. Best is trial 1 with value: 0.37623282358156024.\n",
      "[I 2024-10-31 19:38:18,799] Trial 2 finished with value: 0.2555989583333334 and parameters: {'lr': 8.964048179998964e-05, 'dropout': 0.27441583603393144, 'kernel_size_conv1': 5, 'conv1_channels': 64, 'conv2_channels': 64, 'fc_size': 128, 'batch_size': 32, 'use_batch_norm': True, 'epochs': 29, 'add_l1': True, 'criterion': 'nf_land_rce', 'lambda_l1': 0.0030286525679971347}. Best is trial 1 with value: 0.37623282358156024.\n",
      "[I 2024-10-31 19:43:04,125] Trial 3 finished with value: 0.3825402462121211 and parameters: {'lr': 0.00028429777274509196, 'dropout': 0.23715007925021894, 'kernel_size_conv1': 4, 'conv1_channels': 64, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 32, 'use_batch_norm': False, 'epochs': 22, 'add_l1': True, 'criterion': 'nf_land_rce', 'lambda_l1': 3.268748307699156e-05}. Best is trial 3 with value: 0.3825402462121211.\n",
      "[I 2024-10-31 19:45:40,970] Trial 4 finished with value: 0.24427083333333335 and parameters: {'lr': 0.002120491533095649, 'dropout': 0.1322249159097431, 'kernel_size_conv1': 5, 'conv1_channels': 64, 'conv2_channels': 128, 'fc_size': 256, 'batch_size': 128, 'use_batch_norm': False, 'epochs': 35, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 3 with value: 0.3825402462121211.\n",
      "[I 2024-10-31 19:47:01,051] Trial 5 finished with value: 0.39284375 and parameters: {'lr': 9.813259876435636e-05, 'dropout': 0.34371942770398334, 'kernel_size_conv1': 4, 'conv1_channels': 32, 'conv2_channels': 64, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 25, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 5 with value: 0.39284375.\n",
      "[I 2024-10-31 19:52:29,601] Trial 6 finished with value: 0.39945684523809516 and parameters: {'lr': 1.265045992993833e-05, 'dropout': 0.47939027796085315, 'kernel_size_conv1': 5, 'conv1_channels': 32, 'conv2_channels': 64, 'fc_size': 128, 'batch_size': 32, 'use_batch_norm': True, 'epochs': 35, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 6 with value: 0.39945684523809516.\n",
      "[I 2024-10-31 19:58:26,786] Trial 7 finished with value: 0.39323640046296293 and parameters: {'lr': 0.0001665321365846549, 'dropout': 0.4272449349653688, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': False, 'epochs': 36, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 6 with value: 0.39945684523809516.\n",
      "[I 2024-10-31 20:00:55,814] Trial 8 finished with value: 0.2447916666666666 and parameters: {'lr': 0.005138797228450752, 'dropout': 0.34188106140961066, 'kernel_size_conv1': 3, 'conv1_channels': 32, 'conv2_channels': 64, 'fc_size': 128, 'batch_size': 32, 'use_batch_norm': False, 'epochs': 23, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 6 with value: 0.39945684523809516.\n",
      "[I 2024-10-31 20:02:38,394] Trial 9 finished with value: 0.38110719086021505 and parameters: {'lr': 8.389609332297349e-05, 'dropout': 0.2733086995562869, 'kernel_size_conv1': 5, 'conv1_channels': 32, 'conv2_channels': 128, 'fc_size': 256, 'batch_size': 128, 'use_batch_norm': False, 'epochs': 31, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 6 with value: 0.39945684523809516.\n",
      "[I 2024-10-31 20:10:24,692] Trial 10 finished with value: 0.3848218513257576 and parameters: {'lr': 3.942811506467426e-05, 'dropout': 0.3001837854211953, 'kernel_size_conv1': 3, 'conv1_channels': 32, 'conv2_channels': 128, 'fc_size': 256, 'batch_size': 32, 'use_batch_norm': False, 'epochs': 44, 'add_l1': True, 'criterion': 'nf_land_rce', 'lambda_l1': 8.508004895541849e-05}. Best is trial 6 with value: 0.39945684523809516.\n",
      "[I 2024-10-31 20:14:22,328] Trial 11 finished with value: 0.39002604166666666 and parameters: {'lr': 1.0254034878695877e-05, 'dropout': 0.4997768287781167, 'kernel_size_conv1': 2, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 128, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 40, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 6 with value: 0.39945684523809516.\n",
      "[I 2024-10-31 20:21:22,395] Trial 12 finished with value: 0.4082956414473684 and parameters: {'lr': 1.162126535042371e-05, 'dropout': 0.4935229037040165, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 38, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 12 with value: 0.4082956414473684.\n",
      "[I 2024-10-31 20:28:33,315] Trial 13 finished with value: 0.4060430021367522 and parameters: {'lr': 1.0547532754416358e-05, 'dropout': 0.49116761256400376, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 39, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 12 with value: 0.4082956414473684.\n",
      "[I 2024-10-31 20:35:54,572] Trial 14 finished with value: 0.41550749491869915 and parameters: {'lr': 2.967710247959031e-05, 'dropout': 0.41914734548095073, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 41, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.41550749491869915.\n",
      "[I 2024-10-31 20:44:32,206] Trial 15 finished with value: 0.41027848639455794 and parameters: {'lr': 4.052412566704377e-05, 'dropout': 0.41352893659257084, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 49, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 14 with value: 0.41550749491869915.\n",
      "[I 2024-10-31 20:53:10,166] Trial 16 finished with value: 0.4301525297619047 and parameters: {'lr': 3.5027271666254944e-05, 'dropout': 0.41989665422347583, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 49, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 16 with value: 0.4301525297619047.\n",
      "[I 2024-10-31 21:00:55,531] Trial 17 finished with value: 0.4200520833333334 and parameters: {'lr': 3.0158108443013353e-05, 'dropout': 0.40723824030454947, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 44, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 16 with value: 0.4301525297619047.\n",
      "[I 2024-10-31 21:08:50,986] Trial 18 finished with value: 0.39542824074074073 and parameters: {'lr': 0.0005931179922036776, 'dropout': 0.38135423664599866, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 45, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 16 with value: 0.4301525297619047.\n",
      "[I 2024-10-31 21:31:59,576] Trial 19 finished with value: 0.2483790391156463 and parameters: {'lr': 2.833376049238851e-05, 'dropout': 0.205129299521906, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 49, 'add_l1': True, 'criterion': 'cross_entropy', 'lambda_l1': 0.009972396488647834}. Best is trial 16 with value: 0.4301525297619047.\n",
      "[I 2024-10-31 21:37:34,372] Trial 20 finished with value: 0.38356346899224814 and parameters: {'lr': 0.0005829971315978755, 'dropout': 0.37393230052804693, 'kernel_size_conv1': 2, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 43, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 16 with value: 0.4301525297619047.\n",
      "[I 2024-10-31 21:43:27,432] Trial 21 finished with value: 0.41317368659420295 and parameters: {'lr': 0.00018424272327066766, 'dropout': 0.44886013621960175, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 128, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 46, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 16 with value: 0.4301525297619047.\n",
      "[I 2024-10-31 21:50:45,410] Trial 22 finished with value: 0.41207444105691066 and parameters: {'lr': 2.9689121882844248e-05, 'dropout': 0.4038653935938991, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 41, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 16 with value: 0.4301525297619047.\n",
      "[I 2024-10-31 21:59:45,128] Trial 23 finished with value: 0.42513020833333326 and parameters: {'lr': 2.1144256718859203e-05, 'dropout': 0.443825411455935, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 50, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 16 with value: 0.4301525297619047.\n",
      "[I 2024-10-31 22:08:33,322] Trial 24 finished with value: 0.42133333333333334 and parameters: {'lr': 5.784129165241769e-05, 'dropout': 0.44735395649167276, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 50, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 16 with value: 0.4301525297619047.\n",
      "[I 2024-10-31 22:17:21,380] Trial 25 finished with value: 0.41754427083333334 and parameters: {'lr': 6.323279829635215e-05, 'dropout': 0.4595585236700151, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 50, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 16 with value: 0.4301525297619047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  26\n",
      "Best trial:\n",
      "  Validation Accuracy: 0.4302\n",
      "  Best hyperparameters: \n",
      "    lr: 3.5027271666254944e-05\n",
      "    dropout: 0.41989665422347583\n",
      "    kernel_size_conv1: 4\n",
      "    conv1_channels: 128\n",
      "    conv2_channels: 256\n",
      "    fc_size: 64\n",
      "    batch_size: 64\n",
      "    use_batch_norm: True\n",
      "    epochs: 49\n",
      "    add_l1: False\n",
      "    criterion: cross_entropy\n"
     ]
    }
   ],
   "source": [
    "from utils.tmatrix.transition_matrix import T_MATRIX_MNIST_0_6  # Ensure correct import\n",
    "from utils.models.hyperparametertuning import OptunaOptimization\n",
    "\n",
    "# Define path to your dataset\n",
    "path = 'data/2024_A2_datasets/FashionMNIST0.6.npz'\n",
    "\n",
    "# Initialize the transition matrix\n",
    "t_matrix = T_MATRIX_MNIST_0_6()\n",
    "\n",
    "# Initialize the OptunaOptimization class with a small sample size for testing\n",
    "optimizer = OptunaOptimization(\n",
    "    path=path,\n",
    "    study_name='fashion_mnist_0_6_with_t_matrix',\n",
    "    device=device,\n",
    "    t_matrix=t_matrix,\n",
    "    repetitions=2,\n",
    "    sample_size=0.8  # 0.5% of the data for quick testing\n",
    ")\n",
    "\n",
    "# Run optimization with a reduced number of trials for testing purposes\n",
    "best_trial = optimizer.run_optimization(n_trials=25)  # Start with 10 trials for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-31 22:36:00,268] A new study created in RDB with name: fashion_mnist_0_3_with_t_matrix\n",
      "/Users/niklas/Documents/UNI/Australien/Courses/MachineLearning/Lab1/5328_GroupProject/utils/models/hyperparametertuning.py:48: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
      "/Users/niklas/Documents/UNI/Australien/Courses/MachineLearning/Lab1/5328_GroupProject/utils/models/hyperparametertuning.py:49: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
      "/Users/niklas/Documents/UNI/Australien/Courses/MachineLearning/Lab1/5328_GroupProject/utils/models/hyperparametertuning.py:62: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lambda_l1 = trial.suggest_loguniform('lambda_l1', 1e-5, 1e-2)\n",
      "[I 2024-10-31 22:38:57,575] Trial 0 finished with value: 0.24716222426470597 and parameters: {'lr': 0.0004300025372090388, 'dropout': 0.2566895741873441, 'kernel_size_conv1': 2, 'conv1_channels': 128, 'conv2_channels': 128, 'fc_size': 256, 'batch_size': 128, 'use_batch_norm': False, 'epochs': 34, 'add_l1': True, 'criterion': 'nf_land_rce', 'lambda_l1': 0.0038553349283987656}. Best is trial 0 with value: 0.24716222426470597.\n",
      "[I 2024-10-31 22:41:34,630] Trial 1 finished with value: 0.24134114583333338 and parameters: {'lr': 0.00022224689145275783, 'dropout': 0.37136168241193923, 'kernel_size_conv1': 2, 'conv1_channels': 64, 'conv2_channels': 128, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': False, 'epochs': 44, 'add_l1': True, 'criterion': 'cross_entropy', 'lambda_l1': 0.0018536605064038778}. Best is trial 0 with value: 0.24716222426470597.\n",
      "[I 2024-10-31 22:46:12,455] Trial 2 finished with value: 0.6851483585858585 and parameters: {'lr': 1.0034959851589736e-05, 'dropout': 0.2915172986003681, 'kernel_size_conv1': 5, 'conv1_channels': 128, 'conv2_channels': 128, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 33, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 2 with value: 0.6851483585858585.\n",
      "[I 2024-10-31 22:49:45,461] Trial 3 finished with value: 0.6847688802083333 and parameters: {'lr': 1.2553478905024435e-05, 'dropout': 0.3751784624800253, 'kernel_size_conv1': 3, 'conv1_channels': 64, 'conv2_channels': 128, 'fc_size': 256, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 40, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 2 with value: 0.6851483585858585.\n",
      "[I 2024-10-31 22:51:28,404] Trial 4 finished with value: 0.6113324652777778 and parameters: {'lr': 2.1244423675764534e-05, 'dropout': 0.49780051362845334, 'kernel_size_conv1': 5, 'conv1_channels': 32, 'conv2_channels': 64, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': False, 'epochs': 30, 'add_l1': True, 'criterion': 'nf_land_rce', 'lambda_l1': 0.0014409340812802406}. Best is trial 2 with value: 0.6851483585858585.\n",
      "[I 2024-10-31 22:55:16,529] Trial 5 finished with value: 0.6661005434782609 and parameters: {'lr': 1.1895595140054633e-05, 'dropout': 0.20793406282538332, 'kernel_size_conv1': 4, 'conv1_channels': 32, 'conv2_channels': 256, 'fc_size': 256, 'batch_size': 64, 'use_batch_norm': False, 'epochs': 46, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 2 with value: 0.6851483585858585.\n",
      "[I 2024-10-31 22:58:37,382] Trial 6 finished with value: 0.6830908764367817 and parameters: {'lr': 1.9763418031157376e-05, 'dropout': 0.14602297367785236, 'kernel_size_conv1': 5, 'conv1_channels': 64, 'conv2_channels': 256, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 29, 'add_l1': True, 'criterion': 'nf_land_rce', 'lambda_l1': 1.4622763778370536e-05}. Best is trial 2 with value: 0.6851483585858585.\n",
      "[I 2024-10-31 23:01:14,457] Trial 7 finished with value: 0.25294596354166665 and parameters: {'lr': 4.2133816527945075e-05, 'dropout': 0.28305822722967583, 'kernel_size_conv1': 4, 'conv1_channels': 32, 'conv2_channels': 128, 'fc_size': 128, 'batch_size': 64, 'use_batch_norm': False, 'epochs': 32, 'add_l1': True, 'criterion': 'cross_entropy', 'lambda_l1': 0.002274497931466584}. Best is trial 2 with value: 0.6851483585858585.\n",
      "[I 2024-10-31 23:04:31,415] Trial 8 finished with value: 0.6763569078947368 and parameters: {'lr': 0.00029799638992629007, 'dropout': 0.46967733993943805, 'kernel_size_conv1': 3, 'conv1_channels': 32, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 38, 'add_l1': True, 'criterion': 'cross_entropy', 'lambda_l1': 0.00041850986354976415}. Best is trial 2 with value: 0.6851483585858585.\n",
      "[I 2024-10-31 23:11:23,769] Trial 9 finished with value: 0.40990911989795914 and parameters: {'lr': 0.005442431524135452, 'dropout': 0.18004935570617714, 'kernel_size_conv1': 5, 'conv1_channels': 128, 'conv2_channels': 128, 'fc_size': 256, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 49, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 2 with value: 0.6851483585858585.\n",
      "[I 2024-10-31 23:15:13,781] Trial 10 finished with value: 0.6127781723484849 and parameters: {'lr': 0.007158783067777244, 'dropout': 0.36346243034686293, 'kernel_size_conv1': 5, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 32, 'use_batch_norm': True, 'epochs': 22, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 2 with value: 0.6851483585858585.\n",
      "[I 2024-10-31 23:18:29,926] Trial 11 finished with value: 0.6895499465811964 and parameters: {'lr': 6.785964990303643e-05, 'dropout': 0.37040994197017857, 'kernel_size_conv1': 3, 'conv1_channels': 64, 'conv2_channels': 128, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 39, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 11 with value: 0.6895499465811964.\n",
      "[I 2024-10-31 23:22:33,604] Trial 12 finished with value: 0.6846354166666667 and parameters: {'lr': 6.837406371784385e-05, 'dropout': 0.3280356414319724, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 128, 'fc_size': 64, 'batch_size': 32, 'use_batch_norm': True, 'epochs': 24, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 11 with value: 0.6895499465811964.\n",
      "[I 2024-10-31 23:25:50,063] Trial 13 finished with value: 0.6839710202991451 and parameters: {'lr': 6.662840453109113e-05, 'dropout': 0.4319443678912753, 'kernel_size_conv1': 3, 'conv1_channels': 64, 'conv2_channels': 128, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 39, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 11 with value: 0.6895499465811964.\n",
      "[I 2024-10-31 23:30:43,305] Trial 14 finished with value: 0.6744538483796296 and parameters: {'lr': 0.0009885050769208373, 'dropout': 0.240604863127338, 'kernel_size_conv1': 5, 'conv1_channels': 128, 'conv2_channels': 128, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 36, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 11 with value: 0.6895499465811964.\n",
      "[I 2024-10-31 23:34:56,021] Trial 15 finished with value: 0.6891865079365079 and parameters: {'lr': 0.00012840168427191164, 'dropout': 0.10820596492795903, 'kernel_size_conv1': 3, 'conv1_channels': 64, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 42, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 11 with value: 0.6895499465811964.\n",
      "[I 2024-10-31 23:40:27,584] Trial 16 finished with value: 0.6844597868217053 and parameters: {'lr': 0.00013381125417675526, 'dropout': 0.11181279207163206, 'kernel_size_conv1': 3, 'conv1_channels': 64, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 32, 'use_batch_norm': True, 'epochs': 43, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 11 with value: 0.6895499465811964.\n",
      "[I 2024-10-31 23:45:28,535] Trial 17 finished with value: 0.6756484374999999 and parameters: {'lr': 0.000609289108357644, 'dropout': 0.42112526609101386, 'kernel_size_conv1': 3, 'conv1_channels': 64, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 50, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 11 with value: 0.6895499465811964.\n",
      "[I 2024-10-31 23:49:35,428] Trial 18 finished with value: 0.6596447172619048 and parameters: {'lr': 0.0017986163151638278, 'dropout': 0.31863847455886196, 'kernel_size_conv1': 3, 'conv1_channels': 64, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 42, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 11 with value: 0.6895499465811964.\n",
      "[I 2024-10-31 23:57:03,075] Trial 19 finished with value: 0.6739894701086956 and parameters: {'lr': 0.00014094883216019436, 'dropout': 0.11445733935998488, 'kernel_size_conv1': 3, 'conv1_channels': 64, 'conv2_channels': 256, 'fc_size': 128, 'batch_size': 32, 'use_batch_norm': True, 'epochs': 46, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 11 with value: 0.6895499465811964.\n",
      "[I 2024-11-01 00:00:04,450] Trial 20 finished with value: 0.6815007716049384 and parameters: {'lr': 4.7905242424012074e-05, 'dropout': 0.19654501248038495, 'kernel_size_conv1': 4, 'conv1_channels': 64, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 27, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 11 with value: 0.6895499465811964.\n",
      "[I 2024-11-01 00:03:56,099] Trial 21 finished with value: 0.6870876736111111 and parameters: {'lr': 2.5568756083089732e-05, 'dropout': 0.4189497922324807, 'kernel_size_conv1': 2, 'conv1_channels': 128, 'conv2_channels': 128, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 36, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 11 with value: 0.6895499465811964.\n",
      "[I 2024-11-01 00:07:09,017] Trial 22 finished with value: 0.6920045045045045 and parameters: {'lr': 0.0001148764661772254, 'dropout': 0.4129162544473789, 'kernel_size_conv1': 2, 'conv1_channels': 64, 'conv2_channels': 128, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 37, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 22 with value: 0.6920045045045045.\n",
      "[I 2024-11-01 00:10:36,622] Trial 23 finished with value: 0.6838827489837398 and parameters: {'lr': 0.00011117250465012124, 'dropout': 0.34258789762328296, 'kernel_size_conv1': 2, 'conv1_channels': 64, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 41, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 22 with value: 0.6920045045045045.\n",
      "[I 2024-11-01 00:13:49,090] Trial 24 finished with value: 0.6822916666666667 and parameters: {'lr': 0.00019266301751918143, 'dropout': 0.4023547764300281, 'kernel_size_conv1': 2, 'conv1_channels': 64, 'conv2_channels': 128, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 37, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 22 with value: 0.6920045045045045.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials:  25\n",
      "Best trial:\n",
      "  Validation Accuracy: 0.6920\n",
      "  Best hyperparameters: \n",
      "    lr: 0.0001148764661772254\n",
      "    dropout: 0.4129162544473789\n",
      "    kernel_size_conv1: 2\n",
      "    conv1_channels: 64\n",
      "    conv2_channels: 128\n",
      "    fc_size: 64\n",
      "    batch_size: 64\n",
      "    use_batch_norm: True\n",
      "    epochs: 37\n",
      "    add_l1: False\n",
      "    criterion: nf_land_rce\n"
     ]
    }
   ],
   "source": [
    "from utils.tmatrix.transition_matrix import T_MATRIX_MNIST_0_3  # Ensure correct import\n",
    "from utils.models.hyperparametertuning import OptunaOptimization\n",
    "\n",
    "# Define path to your dataset\n",
    "path = 'data/2024_A2_datasets/FashionMNIST0.3.npz'\n",
    "\n",
    "# Initialize the transition matrix\n",
    "t_matrix = T_MATRIX_MNIST_0_3()\n",
    "\n",
    "# Initialize the OptunaOptimization class with a small sample size for testing\n",
    "optimizer = OptunaOptimization(\n",
    "    path=path,\n",
    "    study_name='fashion_mnist_0_3_with_t_matrix',\n",
    "    device=device,\n",
    "    t_matrix=t_matrix,\n",
    "    repetitions=2,\n",
    "    sample_size=0.8  # 0.5% of the data for quick testing\n",
    ")\n",
    "\n",
    "# Run optimization with a reduced number of trials for testing purposes\n",
    "best_trial = optimizer.run_optimization(n_trials=25)  # Start with 10 trials for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ks/jjk84k5965v08pzg867937440000gn/T/ipykernel_10146/3595797236.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  CNN_mnist_0_3.load_state_dict(torch.load('weights/model0.6920045045045045.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 94.475%\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from utils.models.CNN import CNNModel\n",
    "## laod the best model \n",
    "study = optuna.load_study(study_name='fashion_mnist_0_3_with_t_matrix', storage='sqlite:///cnn_hyperparameter_tuning.db')\n",
    "\n",
    "CNN_mnist_0_3 = CNNModel(t_matrix=t_matrix, \n",
    "                 device=device, \n",
    "                 lr = study.best_params['lr'],\n",
    "                 dropout=study.best_params['dropout'],\n",
    "                 kernel_size_conv = study.best_params['kernel_size_conv1'], \n",
    "                 conv_channels = [study.best_params['conv1_channels'], study.best_params['conv2_channels']],\n",
    "                 use_batch_norm=study.best_params['use_batch_norm'],\n",
    "                 loss = study.best_params['criterion'],\n",
    "                 fc_layers_sizes=[study.best_params['fc_size']],\n",
    "                 input_shape=(1, 28, 28),\n",
    "                 num_classes=4)\n",
    "\n",
    "\n",
    "CNN_mnist_0_3.load_state_dict(torch.load('weights/model0.6920045045045045.pth'))\n",
    "\n",
    "# Evaluate the model\n",
    "CNN_mnist_0_3.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = CNN_mnist_0_3(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 3.5027271666254944e-05, 'dropout': 0.41989665422347583, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 49, 'add_l1': False, 'criterion': 'cross_entropy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ks/jjk84k5965v08pzg867937440000gn/T/ipykernel_10146/441473345.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('weights/model0.4301525297619047.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 56.6%\n"
     ]
    }
   ],
   "source": [
    "# get the study load it in with optuna \n",
    "import optuna\n",
    "\n",
    "study = optuna.load_study(study_name='fashion_mnist_0_6_with_t_matrix', storage='sqlite:///cnn_hyperparameter_tuning.db')\n",
    "\n",
    "\n",
    "print(study.best_params)\n",
    "\n",
    "# evaluate the model with the best parameters on the test dataset \n",
    "from utils.models.CNN import CNNModel\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "model = CNNModel(t_matrix=t_matrix, \n",
    "                 device=device, \n",
    "                 lr = study.best_params['lr'],\n",
    "                 dropout=study.best_params['dropout'],\n",
    "                 kernel_size_conv = study.best_params['kernel_size_conv1'], \n",
    "                 conv_channels = [study.best_params['conv1_channels'], study.best_params['conv2_channels']],\n",
    "                 use_batch_norm=study.best_params['use_batch_norm'],\n",
    "                 loss = study.best_params['criterion'],\n",
    "                 fc_layers_sizes=[study.best_params['fc_size']],\n",
    "                 input_shape=(1, 28, 28),\n",
    "                 num_classes=4)\n",
    "\n",
    "\n",
    "# Load the best weights\n",
    "model.load_state_dict(torch.load('weights/model0.4301525297619047.pth'))\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smaple size: (3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from utils.dataloader.FashionMNISTDataLoader import FashionMNISTDataLoader\n",
    "from utils.models.CNN import CNNModel\n",
    "from utils.tmatrix.transition_matrix import T_MATRIX_CIFAR\n",
    "\n",
    "path_cifar = 'data/2024_A2_datasets/CIFAR10.npz'\n",
    "data_loader_cifar = CIFAR10DataLoader(path=path_cifar, batch_size=64, sample_size=1, train_percentage=0.8, device=device)\n",
    "\n",
    "train_loader, eval_loader, test_loader = data_loader_cifar.get_loaders()\n",
    "\n",
    "print('smaple size:', data_loader_cifar.get_shape_of_sample())\n",
    "\n",
    "\n",
    "CNN_Cifar = CNNModel(t_matrix=T_MATRIX_CIFAR(),\n",
    "                    device=device,\n",
    "                    num_classes=4,\n",
    "                    input_shape=data_loader_cifar.get_shape_of_sample())\n",
    "\n",
    "\n",
    "# CNN_Cifar.fit(train_loader, epochs=20) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-31 14:53:35,090] Using an existing study with name 'CIFAR10' instead of creating a new one.\n",
      "[I 2024-10-31 14:57:08,335] Trial 4 finished with value: 0.772712643678161 and parameters: {'lr': 0.0009536536530709598, 'dropout': 0.10330274730257508, 'kernel_size_conv1': 3, 'conv1_channels': 64, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 29, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 4 with value: 0.772712643678161.\n",
      "[I 2024-10-31 15:08:36,344] Trial 5 finished with value: 0.3469880952380952 and parameters: {'lr': 0.004006172567258268, 'dropout': 0.4976928643440897, 'kernel_size_conv1': 5, 'conv1_channels': 64, 'conv2_channels': 64, 'fc_size': 256, 'batch_size': 32, 'use_batch_norm': True, 'epochs': 49, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 4 with value: 0.772712643678161.\n",
      "[I 2024-10-31 15:16:05,721] Trial 6 finished with value: 0.7122942708333332 and parameters: {'lr': 3.451659818642591e-05, 'dropout': 0.15626229531736824, 'kernel_size_conv1': 4, 'conv1_channels': 64, 'conv2_channels': 256, 'fc_size': 256, 'batch_size': 64, 'use_batch_norm': False, 'epochs': 32, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 4 with value: 0.772712643678161.\n",
      "[I 2024-10-31 15:22:02,429] Trial 7 finished with value: 0.24922222222222226 and parameters: {'lr': 0.004088769380872157, 'dropout': 0.4344053757862383, 'kernel_size_conv1': 5, 'conv1_channels': 64, 'conv2_channels': 128, 'fc_size': 128, 'batch_size': 32, 'use_batch_norm': False, 'epochs': 21, 'add_l1': True, 'criterion': 'cross_entropy', 'lambda_l1': 2.0509371742967548e-05}. Best is trial 4 with value: 0.772712643678161.\n",
      "[I 2024-10-31 15:26:54,260] Trial 8 finished with value: 0.7165133333333333 and parameters: {'lr': 2.1243768488853784e-05, 'dropout': 0.2942874949632521, 'kernel_size_conv1': 2, 'conv1_channels': 64, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 64, 'use_batch_norm': True, 'epochs': 25, 'add_l1': True, 'criterion': 'nf_land_rce', 'lambda_l1': 7.232339959203363e-05}. Best is trial 4 with value: 0.772712643678161.\n",
      "[I 2024-10-31 15:30:42,392] Trial 9 finished with value: 0.6926 and parameters: {'lr': 0.0008285782023054468, 'dropout': 0.45007761565291665, 'kernel_size_conv1': 4, 'conv1_channels': 32, 'conv2_channels': 256, 'fc_size': 128, 'batch_size': 64, 'use_batch_norm': False, 'epochs': 20, 'add_l1': True, 'criterion': 'nf_land_rce', 'lambda_l1': 3.6998761560836746e-05}. Best is trial 4 with value: 0.772712643678161.\n",
      "[I 2024-10-31 15:34:41,658] Trial 10 finished with value: 0.6715277777777776 and parameters: {'lr': 0.002435810256101544, 'dropout': 0.27679270552440965, 'kernel_size_conv1': 3, 'conv1_channels': 32, 'conv2_channels': 128, 'fc_size': 256, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 33, 'add_l1': True, 'criterion': 'nf_land_rce', 'lambda_l1': 0.0007231134383579543}. Best is trial 4 with value: 0.772712643678161.\n",
      "[I 2024-10-31 15:42:25,749] Trial 11 finished with value: 0.7853882113821138 and parameters: {'lr': 0.0007894945933638116, 'dropout': 0.11156181592954598, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 41, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 11 with value: 0.7853882113821138.\n",
      "[I 2024-10-31 15:50:10,489] Trial 12 finished with value: 0.7906829268292683 and parameters: {'lr': 0.0007770430685879307, 'dropout': 0.11041956822175163, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 41, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 12 with value: 0.7906829268292683.\n",
      "[I 2024-10-31 15:58:04,424] Trial 13 finished with value: 0.7914444444444444 and parameters: {'lr': 0.0006693778483483631, 'dropout': 0.20378315365867722, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 42, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 13 with value: 0.7914444444444444.\n",
      "[I 2024-10-31 16:05:20,693] Trial 14 finished with value: 0.7946626016260164 and parameters: {'lr': 8.990258089421415e-05, 'dropout': 0.20227342946154778, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 41, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 16:12:15,769] Trial 15 finished with value: 0.7771709401709399 and parameters: {'lr': 9.478504781303849e-05, 'dropout': 0.21522605810265116, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 39, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 16:20:14,431] Trial 16 finished with value: 0.7821870370370371 and parameters: {'lr': 9.177533613310894e-05, 'dropout': 0.21943543893838383, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 45, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 16:33:40,444] Trial 17 finished with value: 0.2930961538461539 and parameters: {'lr': 0.00950422142768644, 'dropout': 0.34388518833814197, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 64, 'batch_size': 32, 'use_batch_norm': True, 'epochs': 39, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 16:41:28,123] Trial 18 finished with value: 0.7202651515151515 and parameters: {'lr': 1.1451703093959299e-05, 'dropout': 0.23079974274370926, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 44, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 16:47:36,737] Trial 19 finished with value: 0.7760462962962963 and parameters: {'lr': 9.870339977133137e-05, 'dropout': 0.347789145899377, 'kernel_size_conv1': 2, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 36, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 17:04:10,640] Trial 20 finished with value: 0.6571666666666668 and parameters: {'lr': 0.00042219166710874887, 'dropout': 0.16702376247302336, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 256, 'batch_size': 32, 'use_batch_norm': True, 'epochs': 37, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 17:13:09,059] Trial 21 finished with value: 0.7813166666666667 and parameters: {'lr': 5.103345238242022e-05, 'dropout': 0.2517585756181259, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 50, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 17:20:41,194] Trial 22 finished with value: 0.7923650793650794 and parameters: {'lr': 0.0004489330090252505, 'dropout': 0.14100687855417912, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 42, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 17:28:26,129] Trial 23 finished with value: 0.7908081395348837 and parameters: {'lr': 0.0003870703059573136, 'dropout': 0.17508475192192802, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 43, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 17:36:37,086] Trial 24 finished with value: 0.7350634057971014 and parameters: {'lr': 0.0019808650191064977, 'dropout': 0.13853310960088375, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 46, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 17:44:03,699] Trial 25 finished with value: 0.7854126984126983 and parameters: {'lr': 0.00020403713486800672, 'dropout': 0.19726113456163036, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 42, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 17:50:58,630] Trial 26 finished with value: 0.7875961538461538 and parameters: {'lr': 0.0004302617511325013, 'dropout': 0.3335197969715894, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 39, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 17:59:28,015] Trial 27 finished with value: 0.6539237588652482 and parameters: {'lr': 0.0001340078226976447, 'dropout': 0.13888893091749013, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 47, 'add_l1': True, 'criterion': 'nf_land_rce', 'lambda_l1': 0.008483424013584191}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 18:06:25,045] Trial 28 finished with value: 0.771014705882353 and parameters: {'lr': 5.5294545910358066e-05, 'dropout': 0.25275361497949306, 'kernel_size_conv1': 4, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 64, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 34, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 18:19:11,151] Trial 29 finished with value: 0.36468468468468457 and parameters: {'lr': 0.001307961933221718, 'dropout': 0.18582774398042012, 'kernel_size_conv1': 2, 'conv1_channels': 128, 'conv2_channels': 256, 'fc_size': 256, 'batch_size': 32, 'use_batch_norm': True, 'epochs': 37, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 18:25:38,653] Trial 30 finished with value: 0.7775590277777777 and parameters: {'lr': 0.0005382354060430664, 'dropout': 0.13360047112525977, 'kernel_size_conv1': 5, 'conv1_channels': 32, 'conv2_channels': 128, 'fc_size': 128, 'batch_size': 64, 'use_batch_norm': False, 'epochs': 48, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 14 with value: 0.7946626016260164.\n",
      "[I 2024-10-31 18:33:30,001] Trial 31 finished with value: 0.7973977272727272 and parameters: {'lr': 0.0002529819282040766, 'dropout': 0.25720289911166416, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 44, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 31 with value: 0.7973977272727272.\n",
      "[I 2024-10-31 18:41:21,299] Trial 32 finished with value: 0.7962594696969698 and parameters: {'lr': 0.00015483494497033802, 'dropout': 0.2540784239473116, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 44, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 31 with value: 0.7973977272727272.\n",
      "[I 2024-10-31 18:49:12,228] Trial 33 finished with value: 0.7968617424242423 and parameters: {'lr': 0.0002817249690005036, 'dropout': 0.26352907320227736, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 44, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 31 with value: 0.7973977272727272.\n",
      "[I 2024-10-31 18:57:03,286] Trial 34 finished with value: 0.7931287878787879 and parameters: {'lr': 0.00022986473223712988, 'dropout': 0.2603581957517096, 'kernel_size_conv1': 3, 'conv1_channels': 128, 'conv2_channels': 64, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 44, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 31 with value: 0.7973977272727272.\n",
      "[I 2024-10-31 19:00:48,494] Trial 35 finished with value: 0.7218242753623189 and parameters: {'lr': 0.0001460584004627121, 'dropout': 0.3264490730134513, 'kernel_size_conv1': 3, 'conv1_channels': 32, 'conv2_channels': 64, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': False, 'epochs': 46, 'add_l1': False, 'criterion': 'cross_entropy'}. Best is trial 31 with value: 0.7973977272727272.\n",
      "[I 2024-10-31 19:10:05,923] Trial 36 finished with value: 0.8095746527777777 and parameters: {'lr': 0.00023746676972336207, 'dropout': 0.24209237900577477, 'kernel_size_conv1': 2, 'conv1_channels': 128, 'conv2_channels': 128, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 48, 'add_l1': False, 'criterion': 'nf_land_rce'}. Best is trial 36 with value: 0.8095746527777777.\n",
      "[I 2024-10-31 19:19:53,642] Trial 37 finished with value: 0.703725 and parameters: {'lr': 0.00026418498038228587, 'dropout': 0.3123612282269492, 'kernel_size_conv1': 2, 'conv1_channels': 128, 'conv2_channels': 128, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 50, 'add_l1': True, 'criterion': 'cross_entropy', 'lambda_l1': 0.0008707134144739248}. Best is trial 36 with value: 0.8095746527777777.\n",
      "[W 2024-10-31 19:23:41,735] Trial 38 failed with parameters: {'lr': 0.00027510710944483435, 'dropout': 0.3762633760871922, 'kernel_size_conv1': 2, 'conv1_channels': 64, 'conv2_channels': 128, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': False, 'epochs': 48, 'add_l1': False, 'criterion': 'nf_land_rce'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/AdvML/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/Users/niklas/Documents/UNI/Australien/Courses/MachineLearning/Lab1/5328_GroupProject/utils/models/hyperparametertuning.py\", line 137, in objective\n",
      "    running_loss += loss.item()\n",
      "                    ^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-10-31 19:23:41,738] Trial 38 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 23\u001B[0m\n\u001B[1;32m     11\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m OptunaOptimization(\n\u001B[1;32m     12\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[1;32m     13\u001B[0m     study_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCIFAR10\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     19\u001B[0m     sample_shape \u001B[38;5;241m=\u001B[39m data_loader_cifar\u001B[38;5;241m.\u001B[39mget_shape_of_sample()\n\u001B[1;32m     20\u001B[0m )\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Run optimization with a reduced number of trials for testing purposes\u001B[39;00m\n\u001B[0;32m---> 23\u001B[0m best_trial \u001B[38;5;241m=\u001B[39m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_optimization\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Start with 10 trials for testing\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/UNI/Australien/Courses/MachineLearning/Lab1/5328_GroupProject/utils/models/hyperparametertuning.py:190\u001B[0m, in \u001B[0;36mOptunaOptimization.run_optimization\u001B[0;34m(self, n_trials)\u001B[0m\n\u001B[1;32m    182\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(\n\u001B[1;32m    183\u001B[0m     direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m\"\u001B[39m,  \u001B[38;5;66;03m# We aim to maximize validation accuracy\u001B[39;00m\n\u001B[1;32m    184\u001B[0m     study_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstudy_name,\n\u001B[1;32m    185\u001B[0m     storage\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqlite:///cnn_hyperparameter_tuning.db\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    186\u001B[0m     load_if_exists\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    187\u001B[0m )\n\u001B[1;32m    189\u001B[0m \u001B[38;5;66;03m# 2. Start optimization\u001B[39;00m\n\u001B[0;32m--> 190\u001B[0m \u001B[43mstudy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;66;03m# 3. Output the best trial\u001B[39;00m\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of finished trials: \u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28mlen\u001B[39m(study\u001B[38;5;241m.\u001B[39mtrials))\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/AdvML/lib/python3.12/site-packages/optuna/study/study.py:475\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    375\u001B[0m     func: ObjectiveFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    382\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[1;32m    385\u001B[0m \n\u001B[1;32m    386\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 475\u001B[0m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    476\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    477\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    478\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    479\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    480\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    481\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    485\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/AdvML/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001B[0m, in \u001B[0;36m_optimize\u001B[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m---> 63\u001B[0m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/AdvML/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 160\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[1;32m    163\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[1;32m    165\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/AdvML/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    241\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    244\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[1;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    246\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[1;32m    247\u001B[0m ):\n\u001B[0;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/AdvML/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001B[0m, in \u001B[0;36m_run_trial\u001B[0;34m(study, func, catch)\u001B[0m\n\u001B[1;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 197\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[1;32m    200\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "File \u001B[0;32m~/Documents/UNI/Australien/Courses/MachineLearning/Lab1/5328_GroupProject/utils/models/hyperparametertuning.py:137\u001B[0m, in \u001B[0;36mOptunaOptimization.objective\u001B[0;34m(self, trial)\u001B[0m\n\u001B[1;32m    134\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m    135\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m--> 137\u001B[0m     running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;66;03m# 7. Validation after each epoch\u001B[39;00m\n\u001B[1;32m    140\u001B[0m model\u001B[38;5;241m.\u001B[39meval()  \u001B[38;5;66;03m# Set model to evaluation mode\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from utils.tmatrix.transition_matrix import T_MATRIX_CIFAR  # Ensure correct import\n",
    "from utils.models.hyperparametertuning import OptunaOptimization\n",
    "\n",
    "# Define path to your dataset\n",
    "path = 'data/2024_A2_datasets/CIFAR10.npz'  \n",
    "\n",
    "# Initialize the transition matrix\n",
    "t_matrix = T_MATRIX_CIFAR()\n",
    "\n",
    "# Initialize the OptunaOptimization class with a small sample size for testing\n",
    "optimizer = OptunaOptimization(\n",
    "    path=path,\n",
    "    study_name='CIFAR10',\n",
    "    weights_path='weights/CIFAR10_test',\n",
    "    device=device,\n",
    "    t_matrix=t_matrix,\n",
    "    repetitions=3,\n",
    "    sample_size=1,  # 0.5% of the data for quick testing\n",
    "    sample_shape = data_loader_cifar.get_shape_of_sample()\n",
    ")\n",
    "\n",
    "# Run optimization with a reduced number of trials for testing purposes\n",
    "best_trial = optimizer.run_optimization(n_trials=50)  # Start with 10 trials for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.00023746676972336207, 'dropout': 0.24209237900577477, 'kernel_size_conv1': 2, 'conv1_channels': 128, 'conv2_channels': 128, 'fc_size': 128, 'batch_size': 128, 'use_batch_norm': True, 'epochs': 48, 'add_l1': False, 'criterion': 'nf_land_rce'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ks/jjk84k5965v08pzg867937440000gn/T/ipykernel_10146/4241142323.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  CNN_Cifar.load_state_dict(torch.load('weights/CIFAR10_test/model0.8095746527777777.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 89.1125%\n"
     ]
    }
   ],
   "source": [
    "import optuna \n",
    "\n",
    "# import the best parameters\n",
    "study = optuna.load_study(study_name='CIFAR10', storage='sqlite:///cnn_hyperparameter_tuning.db')\n",
    "\n",
    "print(study.best_params)\n",
    "\n",
    "# eval the model\n",
    "\n",
    "# Initialize the model with the best parameters\n",
    "\n",
    "CNN_Cifar = CNNModel(t_matrix=T_MATRIX_CIFAR(),\n",
    "                    device=device,\n",
    "                    num_classes=4,\n",
    "                    input_shape=data_loader_cifar.get_shape_of_sample(),\n",
    "                    lr = study.best_params['lr'],\n",
    "                    dropout=study.best_params['dropout'],\n",
    "                    kernel_size_conv = study.best_params['kernel_size_conv1'], \n",
    "                    conv_channels = [study.best_params['conv1_channels'], study.best_params['conv2_channels']],\n",
    "                    use_batch_norm=study.best_params['use_batch_norm'],\n",
    "                    use_transition_matrix=True,\n",
    "                    loss = study.best_params['criterion'],\n",
    "                    fc_layers_sizes=[study.best_params['fc_size']])\n",
    "\n",
    "# Load the best weights\n",
    "CNN_Cifar.load_state_dict(torch.load('weights/CIFAR10_test/model0.8095746527777777.pth'))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "CNN_Cifar.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = CNN_Cifar(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total}%')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MLP"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:21:31.429487Z",
     "start_time": "2024-11-03T11:21:31.355111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset,Subset, DataLoader, TensorDataset"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:21:31.738742Z",
     "start_time": "2024-11-03T11:21:31.736261Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:21:32.801450Z",
     "start_time": "2024-11-03T11:21:32.794800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def T_MATRIX_MNIST_0_6():\n",
    "    return torch.tensor([\n",
    "        [0.4, 0.2, 0.2, 0.2],\n",
    "        [0.2, 0.4, 0.2, 0.2],\n",
    "        [0.2, 0.2, 0.4, 0.2],\n",
    "        [0.2, 0.2, 0.2, 0.4]\n",
    "    ]).to(device)\n",
    "\n",
    "def T_MATRIX_MNIST_0_3():\n",
    "    return torch.tensor([\n",
    "        [0.7, 0.3, 0, 0],\n",
    "        [0, 0.7, 0.3, 0],\n",
    "        [0, 0, 0.7, 0.3],\n",
    "        [0.3, 0, 0, 0.7]\n",
    "    ]).to(device)\n",
    "\n",
    "def T_MATRIX_CIFAR10_1():\n",
    "    return torch.tensor([[0.86189902, 0.10806329, 0.02108065, 0.00895717],\n",
    " [0.01167643, 0.90845531, 0.07811296, 0.00175436],\n",
    " [0.02824813, 0.02348635, 0.85186839, 0.09639635],\n",
    " [0.08581234, 0.02207433, 0.06435744, 0.8277564 ]]).to(device)\n",
    "\n",
    "def T_MATRIX_CIFAR10_2():\n",
    "    return torch.tensor([[0.75765127, 0.19187541, 0.03330756, 0.0171661 ], \n",
    "                         [0.0157136, 0.87488556, 0.10403936, 0.00536188], \n",
    "                         [0.03524161, 0.0154408, 0.80679089, 0.14252695], \n",
    "                         [0.13659286, 0.01401313, 0.07197482, 0.7774201 ]]).to(device)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:21:33.484367Z",
     "start_time": "2024-11-03T11:21:33.471584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ClassifierMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p=0.5):\n",
    "        super(ClassifierMLP, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input tensor to a 1D vector\n",
    "        x = x.view(-1, self.input_dim)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "def noise_corrected_loss(output, labels, transition_matrix):\n",
    "    # Create a criterion that calculates the CrossEntropyLoss for each element\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    # Apply the criterion to get the initial loss values\n",
    "    loss = criterion(output, labels)\n",
    "\n",
    "    # Apply softmax to the output to get probability distributions\n",
    "    softmax_output = torch.softmax(output, dim=-1)\n",
    "    # Correct the predictions based on the transition matrix\n",
    "    corrected_output = torch.matmul(softmax_output, transition_matrix)\n",
    "    # Calculate the corrected loss using negative log likelihood\n",
    "    corrected_loss = -torch.log(corrected_output[range(len(labels)), labels])\n",
    "    # Return the mean of the corrected losses\n",
    "    return torch.mean(corrected_loss)\n",
    "\n",
    "def train_model(model, train_loader,val_loader, transition_matrix, num_epochs=10, learning_rate=1e-3, device=device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    accuracy_list = []\n",
    "    val_acc_list = []\n",
    "    loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        for data, labels in train_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            loss = noise_corrected_loss(output, labels, transition_matrix)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(output, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        loss_list.append(avg_loss)\n",
    "\n",
    "        # Calculate accuracy for the epoch\n",
    "        accuracy = accuracy_score(all_labels, all_predictions)\n",
    "        accuracy_list.append(accuracy)\n",
    "\n",
    "        val_acc = validate_model(model, val_loader, device)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/100:.4f}, val: {val_acc:.4f}\")\n",
    "\n",
    "        \n",
    "\n",
    "    return accuracy_list, loss_list, val_acc_list\n",
    "\n",
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, labels in val_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = correct / total\n",
    "    return val_accuracy"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:21:33.991866Z",
     "start_time": "2024-11-03T11:21:33.984426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fashionDataLoader(path, percentage=0.1, batch_size=128, device='cpu' ):\n",
    "    dataset_ = np.load(path)\n",
    "    \n",
    "    X_train_ = dataset_['X_tr']\n",
    "    S_train_ = dataset_['S_tr']\n",
    "    X_test_ = dataset_['X_ts']\n",
    "    Y_test_ = dataset_['Y_ts'] \n",
    "    \n",
    "    output_dim = np.unique(S_train_).shape[0]\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    n_samples_ = int(percentage * X_train_.shape[0])\n",
    "    train_indices_ = np.random.choice(X_train_.shape[0], n_samples_, replace=False)\n",
    "    val_indices_ = np.setdiff1d(np.arange(X_train_.shape[0]), train_indices_)\n",
    "\n",
    "    #train set\n",
    "    X_train_selected = X_train_[train_indices_]\n",
    "    S_train_selected = S_train_[train_indices_]\n",
    "\n",
    "    #validation set\n",
    "    X_val_selected = X_train_[val_indices_]\n",
    "    S_val_selected = S_train_[val_indices_]\n",
    "\n",
    "\n",
    "    # Convert NumPy arrays to PyTorch tensors and move to device\n",
    "    X_train_tensor = torch.tensor(X_train_selected, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    X_train_tensor = (X_train_tensor - X_train_tensor.mean()) / X_train_tensor.std()\n",
    "    S_train_tensor = torch.tensor(S_train_selected, dtype=torch.long).to(device)\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val_selected, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    X_val_tensor = (X_val_tensor - X_val_tensor.mean()) / X_val_tensor.std()\n",
    "    S_val_tensor = torch.tensor(S_val_selected, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create TensorDataset and DataLoader\n",
    "    train_dataset_ = TensorDataset(X_train_tensor, S_train_tensor)\n",
    "    train_loader_ = DataLoader(train_dataset_, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = TensorDataset(X_val_tensor, S_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    ### TEST SET ###\n",
    "    \n",
    "    # Convert NumPy arrays to PyTorch tensors and move to device\n",
    "    X_test_tensor_ = torch.tensor(X_test_, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "    X_test_tensor_ = (X_test_tensor_ - X_test_tensor_.mean()) / X_test_tensor_.std()\n",
    "    Y_test_tensor_ = torch.tensor(Y_test_, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create TensorDataset and DataLoader\n",
    "    test_dataset_ = TensorDataset(X_test_tensor_, Y_test_tensor_)\n",
    "    test_loader_ = DataLoader(test_dataset_, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader_, test_loader_, val_loader, output_dim"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:21:34.472211Z",
     "start_time": "2024-11-03T11:21:34.467545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model_fun(input_dim, hidden_dim, output_dim, dropout_p=0.5, train_loader=None, val_loader=None, transition_matrix = None, num_epochs=50, device=device):\n",
    "    train_loader = train_loader\n",
    "    val_loader = val_loader\n",
    "    model = ClassifierMLP(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_p=dropout_p).to(device)\n",
    "    accuracy_list, loss_list, val_acc_list = train_model(model, \n",
    "                                           train_loader, \n",
    "                                           val_loader,\n",
    "                                           transition_matrix=transition_matrix, \n",
    "                                           num_epochs=num_epochs, \n",
    "                                           device=device)\n",
    "    return model, accuracy_list, loss_list, val_acc_list\n",
    "    \n",
    "def evaluate_model_fun(model, test_loader, device):\n",
    "    # Evaluate the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = correct / total\n",
    "    #print(f'Accuracy of the model on the test images: {100 * correct / total}%')\n",
    "    return test_accuracy"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:21:34.930742Z",
     "start_time": "2024-11-03T11:21:34.926382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_multiple_experiments(path, num_experiments, input_dim, hidden_dim, dropout_p=0.5, transition_matrix=None, num_epochs=50, percentage=0.8, batch_size=128, device='cpu'):\n",
    "    all_accuracies = []\n",
    "    experiment_results = []\n",
    "    val_accuracy_list = []\n",
    "    test_accuracy_list = []\n",
    "    file_name = path.split('/')[-1]\n",
    "    dataset_name = file_name.split('.')[:2]\n",
    "\n",
    "    train_loader, test_loader, val_loader, output_dim = fashionDataLoader(path, \n",
    "                                                                          percentage=percentage, \n",
    "                                                                          batch_size=batch_size, \n",
    "                                                                          device=device)\n",
    "    for i in range(num_experiments):\n",
    "        \n",
    "        model, accuracy_list, loss_list, val_acc_list = train_model_fun(\n",
    "        input_dim = input_dim,\n",
    "        hidden_dim = hidden_dim,\n",
    "        output_dim= output_dim,\n",
    "        dropout_p = dropout_p,\n",
    "        train_loader = train_loader,\n",
    "        val_loader= val_loader,\n",
    "        transition_matrix = transition_matrix,\n",
    "        num_epochs = num_epochs,\n",
    "        device = device\n",
    "        )\n",
    "\n",
    "        val_acc = np.mean(val_acc_list)\n",
    "\n",
    "        test_accuracy = evaluate_model_fun(model, test_loader, device)\n",
    "\n",
    "        test_accuracy_list.append(test_accuracy)\n",
    "        val_accuracy_list.append(val_acc)\n",
    "\n",
    "    mean_val_accuracy = np.mean(val_accuracy_list)\n",
    "    mean_accuracy = np.mean(test_accuracy_list)\n",
    "    std_accuracy = np.std(test_accuracy_list)\n",
    "\n",
    "    experiment_results.append({\n",
    "        'dataset_name': dataset_name,\n",
    "        'mean_val_accuracy': mean_val_accuracy,\n",
    "        'mean_accuracy': mean_accuracy,\n",
    "        'std_accuracy': std_accuracy\n",
    "    })\n",
    "\n",
    "    results_df = pd.DataFrame(experiment_results)\n",
    "    return results_df"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:31:29.863666Z",
     "start_time": "2024-11-03T11:21:35.362428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results_df_03 = run_multiple_experiments(\n",
    "    path='data/2024_A2_datasets/FashionMNIST0.3.npz', \n",
    "    num_experiments=10,\n",
    "    input_dim=28*28, \n",
    "    hidden_dim=64, \n",
    "    transition_matrix=T_MATRIX_MNIST_0_3(), \n",
    "    num_epochs=50, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(results_df_03)\n",
    "\n",
    "results_df_06 = run_multiple_experiments(\n",
    "    path='data/2024_A2_datasets/FashionMNIST0.6.npz', \n",
    "    num_experiments=10,\n",
    "    input_dim=28*28, \n",
    "    hidden_dim=64, \n",
    "    transition_matrix=T_MATRIX_MNIST_0_6(), \n",
    "    num_epochs=50, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(results_df_06)\n",
    "\n",
    "results_df_cifa = run_multiple_experiments(\n",
    "    path='data/2024_A2_datasets/CIFAR10.npz', \n",
    "    num_experiments=10,\n",
    "    input_dim=32*32*3, \n",
    "    hidden_dim=256, \n",
    "    transition_matrix=T_MATRIX_CIFAR10_2(), \n",
    "    num_epochs=50, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "result = pd.concat([results_df_03, results_df_06, results_df_cifa])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.2642, val: 0.6675\n",
      "Epoch 2/50, Loss: 1.1030, val: 0.6769\n",
      "Epoch 3/50, Loss: 1.0822, val: 0.6756\n",
      "Epoch 4/50, Loss: 1.0672, val: 0.6763\n",
      "Epoch 5/50, Loss: 1.0573, val: 0.6787\n",
      "Epoch 6/50, Loss: 1.0434, val: 0.6817\n",
      "Epoch 7/50, Loss: 1.0334, val: 0.6802\n",
      "Epoch 8/50, Loss: 1.0316, val: 0.6806\n",
      "Epoch 9/50, Loss: 1.0175, val: 0.6835\n",
      "Epoch 10/50, Loss: 1.0081, val: 0.6781\n",
      "Epoch 11/50, Loss: 1.0047, val: 0.6840\n",
      "Epoch 12/50, Loss: 0.9958, val: 0.6777\n",
      "Epoch 13/50, Loss: 0.9953, val: 0.6817\n",
      "Epoch 14/50, Loss: 0.9889, val: 0.6783\n",
      "Epoch 15/50, Loss: 0.9851, val: 0.6846\n",
      "Epoch 16/50, Loss: 0.9812, val: 0.6856\n",
      "Epoch 17/50, Loss: 0.9745, val: 0.6823\n",
      "Epoch 18/50, Loss: 0.9710, val: 0.6800\n",
      "Epoch 19/50, Loss: 0.9699, val: 0.6850\n",
      "Epoch 20/50, Loss: 0.9675, val: 0.6819\n",
      "Epoch 21/50, Loss: 0.9633, val: 0.6775\n",
      "Epoch 22/50, Loss: 0.9575, val: 0.6802\n",
      "Epoch 23/50, Loss: 0.9544, val: 0.6833\n",
      "Epoch 24/50, Loss: 0.9510, val: 0.6825\n",
      "Epoch 25/50, Loss: 0.9471, val: 0.6731\n",
      "Epoch 26/50, Loss: 0.9475, val: 0.6773\n",
      "Epoch 27/50, Loss: 0.9458, val: 0.6829\n",
      "Epoch 28/50, Loss: 0.9397, val: 0.6817\n",
      "Epoch 29/50, Loss: 0.9424, val: 0.6823\n",
      "Epoch 30/50, Loss: 0.9379, val: 0.6844\n",
      "Epoch 31/50, Loss: 0.9383, val: 0.6829\n",
      "Epoch 32/50, Loss: 0.9335, val: 0.6850\n",
      "Epoch 33/50, Loss: 0.9309, val: 0.6802\n",
      "Epoch 34/50, Loss: 0.9295, val: 0.6802\n",
      "Epoch 35/50, Loss: 0.9292, val: 0.6775\n",
      "Epoch 36/50, Loss: 0.9316, val: 0.6833\n",
      "Epoch 37/50, Loss: 0.9280, val: 0.6827\n",
      "Epoch 38/50, Loss: 0.9250, val: 0.6808\n",
      "Epoch 39/50, Loss: 0.9219, val: 0.6813\n",
      "Epoch 40/50, Loss: 0.9257, val: 0.6792\n",
      "Epoch 41/50, Loss: 0.9202, val: 0.6813\n",
      "Epoch 42/50, Loss: 0.9209, val: 0.6815\n",
      "Epoch 43/50, Loss: 0.9221, val: 0.6798\n",
      "Epoch 44/50, Loss: 0.9198, val: 0.6846\n",
      "Epoch 45/50, Loss: 0.9165, val: 0.6806\n",
      "Epoch 46/50, Loss: 0.9147, val: 0.6823\n",
      "Epoch 47/50, Loss: 0.9188, val: 0.6808\n",
      "Epoch 48/50, Loss: 0.9151, val: 0.6817\n",
      "Epoch 49/50, Loss: 0.9181, val: 0.6794\n",
      "Epoch 50/50, Loss: 0.9107, val: 0.6808\n",
      "Epoch 1/50, Loss: 1.2627, val: 0.6725\n",
      "Epoch 2/50, Loss: 1.1034, val: 0.6746\n",
      "Epoch 3/50, Loss: 1.0807, val: 0.6773\n",
      "Epoch 4/50, Loss: 1.0656, val: 0.6825\n",
      "Epoch 5/50, Loss: 1.0516, val: 0.6798\n",
      "Epoch 6/50, Loss: 1.0455, val: 0.6831\n",
      "Epoch 7/50, Loss: 1.0321, val: 0.6806\n",
      "Epoch 8/50, Loss: 1.0252, val: 0.6835\n",
      "Epoch 9/50, Loss: 1.0201, val: 0.6825\n",
      "Epoch 10/50, Loss: 1.0108, val: 0.6850\n",
      "Epoch 11/50, Loss: 1.0057, val: 0.6827\n",
      "Epoch 12/50, Loss: 1.0002, val: 0.6852\n",
      "Epoch 13/50, Loss: 0.9955, val: 0.6815\n",
      "Epoch 14/50, Loss: 0.9894, val: 0.6831\n",
      "Epoch 15/50, Loss: 0.9830, val: 0.6817\n",
      "Epoch 16/50, Loss: 0.9795, val: 0.6852\n",
      "Epoch 17/50, Loss: 0.9753, val: 0.6844\n",
      "Epoch 18/50, Loss: 0.9744, val: 0.6840\n",
      "Epoch 19/50, Loss: 0.9678, val: 0.6819\n",
      "Epoch 20/50, Loss: 0.9649, val: 0.6817\n",
      "Epoch 21/50, Loss: 0.9590, val: 0.6848\n",
      "Epoch 22/50, Loss: 0.9613, val: 0.6854\n",
      "Epoch 23/50, Loss: 0.9562, val: 0.6829\n",
      "Epoch 24/50, Loss: 0.9516, val: 0.6798\n",
      "Epoch 25/50, Loss: 0.9507, val: 0.6844\n",
      "Epoch 26/50, Loss: 0.9469, val: 0.6833\n",
      "Epoch 27/50, Loss: 0.9444, val: 0.6829\n",
      "Epoch 28/50, Loss: 0.9429, val: 0.6835\n",
      "Epoch 29/50, Loss: 0.9417, val: 0.6852\n",
      "Epoch 30/50, Loss: 0.9383, val: 0.6850\n",
      "Epoch 31/50, Loss: 0.9346, val: 0.6856\n",
      "Epoch 32/50, Loss: 0.9352, val: 0.6860\n",
      "Epoch 33/50, Loss: 0.9311, val: 0.6850\n",
      "Epoch 34/50, Loss: 0.9312, val: 0.6810\n",
      "Epoch 35/50, Loss: 0.9290, val: 0.6819\n",
      "Epoch 36/50, Loss: 0.9279, val: 0.6854\n",
      "Epoch 37/50, Loss: 0.9297, val: 0.6860\n",
      "Epoch 38/50, Loss: 0.9233, val: 0.6825\n",
      "Epoch 39/50, Loss: 0.9315, val: 0.6842\n",
      "Epoch 40/50, Loss: 0.9202, val: 0.6840\n",
      "Epoch 41/50, Loss: 0.9251, val: 0.6823\n",
      "Epoch 42/50, Loss: 0.9180, val: 0.6833\n",
      "Epoch 43/50, Loss: 0.9240, val: 0.6854\n",
      "Epoch 44/50, Loss: 0.9182, val: 0.6827\n",
      "Epoch 45/50, Loss: 0.9207, val: 0.6821\n",
      "Epoch 46/50, Loss: 0.9156, val: 0.6825\n",
      "Epoch 47/50, Loss: 0.9175, val: 0.6813\n",
      "Epoch 48/50, Loss: 0.9146, val: 0.6819\n",
      "Epoch 49/50, Loss: 0.9126, val: 0.6831\n",
      "Epoch 50/50, Loss: 0.9121, val: 0.6802\n",
      "Epoch 1/50, Loss: 1.2874, val: 0.6690\n",
      "Epoch 2/50, Loss: 1.1053, val: 0.6767\n",
      "Epoch 3/50, Loss: 1.0808, val: 0.6785\n",
      "Epoch 4/50, Loss: 1.0672, val: 0.6737\n",
      "Epoch 5/50, Loss: 1.0538, val: 0.6754\n",
      "Epoch 6/50, Loss: 1.0403, val: 0.6802\n",
      "Epoch 7/50, Loss: 1.0329, val: 0.6821\n",
      "Epoch 8/50, Loss: 1.0231, val: 0.6835\n",
      "Epoch 9/50, Loss: 1.0167, val: 0.6829\n",
      "Epoch 10/50, Loss: 1.0071, val: 0.6865\n",
      "Epoch 11/50, Loss: 1.0031, val: 0.6827\n",
      "Epoch 12/50, Loss: 0.9932, val: 0.6813\n",
      "Epoch 13/50, Loss: 0.9899, val: 0.6867\n",
      "Epoch 14/50, Loss: 0.9862, val: 0.6854\n",
      "Epoch 15/50, Loss: 0.9803, val: 0.6806\n",
      "Epoch 16/50, Loss: 0.9761, val: 0.6844\n",
      "Epoch 17/50, Loss: 0.9686, val: 0.6829\n",
      "Epoch 18/50, Loss: 0.9681, val: 0.6848\n",
      "Epoch 19/50, Loss: 0.9633, val: 0.6827\n",
      "Epoch 20/50, Loss: 0.9610, val: 0.6835\n",
      "Epoch 21/50, Loss: 0.9610, val: 0.6842\n",
      "Epoch 22/50, Loss: 0.9554, val: 0.6817\n",
      "Epoch 23/50, Loss: 0.9545, val: 0.6865\n",
      "Epoch 24/50, Loss: 0.9516, val: 0.6856\n",
      "Epoch 25/50, Loss: 0.9465, val: 0.6856\n",
      "Epoch 26/50, Loss: 0.9440, val: 0.6842\n",
      "Epoch 27/50, Loss: 0.9413, val: 0.6837\n",
      "Epoch 28/50, Loss: 0.9386, val: 0.6869\n",
      "Epoch 29/50, Loss: 0.9404, val: 0.6796\n",
      "Epoch 30/50, Loss: 0.9375, val: 0.6852\n",
      "Epoch 31/50, Loss: 0.9338, val: 0.6850\n",
      "Epoch 32/50, Loss: 0.9299, val: 0.6856\n",
      "Epoch 33/50, Loss: 0.9283, val: 0.6831\n",
      "Epoch 34/50, Loss: 0.9308, val: 0.6821\n",
      "Epoch 35/50, Loss: 0.9291, val: 0.6844\n",
      "Epoch 36/50, Loss: 0.9284, val: 0.6819\n",
      "Epoch 37/50, Loss: 0.9244, val: 0.6808\n",
      "Epoch 38/50, Loss: 0.9266, val: 0.6835\n",
      "Epoch 39/50, Loss: 0.9250, val: 0.6808\n",
      "Epoch 40/50, Loss: 0.9227, val: 0.6846\n",
      "Epoch 41/50, Loss: 0.9225, val: 0.6794\n",
      "Epoch 42/50, Loss: 0.9179, val: 0.6837\n",
      "Epoch 43/50, Loss: 0.9167, val: 0.6833\n",
      "Epoch 44/50, Loss: 0.9149, val: 0.6777\n",
      "Epoch 45/50, Loss: 0.9189, val: 0.6833\n",
      "Epoch 46/50, Loss: 0.9153, val: 0.6827\n",
      "Epoch 47/50, Loss: 0.9227, val: 0.6825\n",
      "Epoch 48/50, Loss: 0.9262, val: 0.6804\n",
      "Epoch 49/50, Loss: 0.9155, val: 0.6800\n",
      "Epoch 50/50, Loss: 0.9131, val: 0.6829\n",
      "Epoch 1/50, Loss: 1.2932, val: 0.6671\n",
      "Epoch 2/50, Loss: 1.1076, val: 0.6715\n",
      "Epoch 3/50, Loss: 1.0874, val: 0.6796\n",
      "Epoch 4/50, Loss: 1.0719, val: 0.6779\n",
      "Epoch 5/50, Loss: 1.0631, val: 0.6802\n",
      "Epoch 6/50, Loss: 1.0529, val: 0.6821\n",
      "Epoch 7/50, Loss: 1.0424, val: 0.6842\n",
      "Epoch 8/50, Loss: 1.0317, val: 0.6846\n",
      "Epoch 9/50, Loss: 1.0253, val: 0.6813\n",
      "Epoch 10/50, Loss: 1.0186, val: 0.6821\n",
      "Epoch 11/50, Loss: 1.0104, val: 0.6842\n",
      "Epoch 12/50, Loss: 1.0055, val: 0.6827\n",
      "Epoch 13/50, Loss: 1.0022, val: 0.6808\n",
      "Epoch 14/50, Loss: 0.9962, val: 0.6821\n",
      "Epoch 15/50, Loss: 0.9900, val: 0.6819\n",
      "Epoch 16/50, Loss: 0.9858, val: 0.6796\n",
      "Epoch 17/50, Loss: 0.9811, val: 0.6837\n",
      "Epoch 18/50, Loss: 0.9795, val: 0.6821\n",
      "Epoch 19/50, Loss: 0.9745, val: 0.6829\n",
      "Epoch 20/50, Loss: 0.9725, val: 0.6783\n",
      "Epoch 21/50, Loss: 0.9648, val: 0.6835\n",
      "Epoch 22/50, Loss: 0.9630, val: 0.6819\n",
      "Epoch 23/50, Loss: 0.9592, val: 0.6835\n",
      "Epoch 24/50, Loss: 0.9588, val: 0.6827\n",
      "Epoch 25/50, Loss: 0.9543, val: 0.6804\n",
      "Epoch 26/50, Loss: 0.9488, val: 0.6787\n",
      "Epoch 27/50, Loss: 0.9511, val: 0.6823\n",
      "Epoch 28/50, Loss: 0.9475, val: 0.6819\n",
      "Epoch 29/50, Loss: 0.9458, val: 0.6837\n",
      "Epoch 30/50, Loss: 0.9434, val: 0.6815\n",
      "Epoch 31/50, Loss: 0.9444, val: 0.6815\n",
      "Epoch 32/50, Loss: 0.9371, val: 0.6813\n",
      "Epoch 33/50, Loss: 0.9376, val: 0.6800\n",
      "Epoch 34/50, Loss: 0.9325, val: 0.6815\n",
      "Epoch 35/50, Loss: 0.9346, val: 0.6831\n",
      "Epoch 36/50, Loss: 0.9332, val: 0.6817\n",
      "Epoch 37/50, Loss: 0.9291, val: 0.6825\n",
      "Epoch 38/50, Loss: 0.9279, val: 0.6817\n",
      "Epoch 39/50, Loss: 0.9244, val: 0.6800\n",
      "Epoch 40/50, Loss: 0.9256, val: 0.6800\n",
      "Epoch 41/50, Loss: 0.9285, val: 0.6819\n",
      "Epoch 42/50, Loss: 0.9214, val: 0.6802\n",
      "Epoch 43/50, Loss: 0.9231, val: 0.6808\n",
      "Epoch 44/50, Loss: 0.9183, val: 0.6817\n",
      "Epoch 45/50, Loss: 0.9182, val: 0.6798\n",
      "Epoch 46/50, Loss: 0.9269, val: 0.6810\n",
      "Epoch 47/50, Loss: 0.9205, val: 0.6804\n",
      "Epoch 48/50, Loss: 0.9176, val: 0.6808\n",
      "Epoch 49/50, Loss: 0.9168, val: 0.6806\n",
      "Epoch 50/50, Loss: 0.9119, val: 0.6808\n",
      "Epoch 1/50, Loss: 1.2676, val: 0.6696\n",
      "Epoch 2/50, Loss: 1.0994, val: 0.6787\n",
      "Epoch 3/50, Loss: 1.0823, val: 0.6729\n",
      "Epoch 4/50, Loss: 1.0677, val: 0.6756\n",
      "Epoch 5/50, Loss: 1.0608, val: 0.6785\n",
      "Epoch 6/50, Loss: 1.0427, val: 0.6833\n",
      "Epoch 7/50, Loss: 1.0338, val: 0.6823\n",
      "Epoch 8/50, Loss: 1.0269, val: 0.6817\n",
      "Epoch 9/50, Loss: 1.0167, val: 0.6833\n",
      "Epoch 10/50, Loss: 1.0092, val: 0.6815\n",
      "Epoch 11/50, Loss: 1.0028, val: 0.6840\n",
      "Epoch 12/50, Loss: 1.0027, val: 0.6831\n",
      "Epoch 13/50, Loss: 0.9900, val: 0.6833\n",
      "Epoch 14/50, Loss: 0.9860, val: 0.6833\n",
      "Epoch 15/50, Loss: 0.9815, val: 0.6833\n",
      "Epoch 16/50, Loss: 0.9745, val: 0.6840\n",
      "Epoch 17/50, Loss: 0.9713, val: 0.6825\n",
      "Epoch 18/50, Loss: 0.9697, val: 0.6846\n",
      "Epoch 19/50, Loss: 0.9654, val: 0.6819\n",
      "Epoch 20/50, Loss: 0.9633, val: 0.6844\n",
      "Epoch 21/50, Loss: 0.9588, val: 0.6827\n",
      "Epoch 22/50, Loss: 0.9564, val: 0.6844\n",
      "Epoch 23/50, Loss: 0.9535, val: 0.6846\n",
      "Epoch 24/50, Loss: 0.9495, val: 0.6827\n",
      "Epoch 25/50, Loss: 0.9508, val: 0.6850\n",
      "Epoch 26/50, Loss: 0.9439, val: 0.6821\n",
      "Epoch 27/50, Loss: 0.9440, val: 0.6865\n",
      "Epoch 28/50, Loss: 0.9398, val: 0.6810\n",
      "Epoch 29/50, Loss: 0.9371, val: 0.6854\n",
      "Epoch 30/50, Loss: 0.9367, val: 0.6792\n",
      "Epoch 31/50, Loss: 0.9331, val: 0.6835\n",
      "Epoch 32/50, Loss: 0.9322, val: 0.6829\n",
      "Epoch 33/50, Loss: 0.9345, val: 0.6850\n",
      "Epoch 34/50, Loss: 0.9315, val: 0.6827\n",
      "Epoch 35/50, Loss: 0.9282, val: 0.6837\n",
      "Epoch 36/50, Loss: 0.9311, val: 0.6819\n",
      "Epoch 37/50, Loss: 0.9299, val: 0.6808\n",
      "Epoch 38/50, Loss: 0.9267, val: 0.6806\n",
      "Epoch 39/50, Loss: 0.9235, val: 0.6817\n",
      "Epoch 40/50, Loss: 0.9213, val: 0.6823\n",
      "Epoch 41/50, Loss: 0.9189, val: 0.6806\n",
      "Epoch 42/50, Loss: 0.9217, val: 0.6833\n",
      "Epoch 43/50, Loss: 0.9250, val: 0.6813\n",
      "Epoch 44/50, Loss: 0.9194, val: 0.6825\n",
      "Epoch 45/50, Loss: 0.9185, val: 0.6831\n",
      "Epoch 46/50, Loss: 0.9164, val: 0.6844\n",
      "Epoch 47/50, Loss: 0.9177, val: 0.6835\n",
      "Epoch 48/50, Loss: 0.9132, val: 0.6823\n",
      "Epoch 49/50, Loss: 0.9151, val: 0.6800\n",
      "Epoch 50/50, Loss: 0.9145, val: 0.6796\n",
      "Epoch 1/50, Loss: 1.3174, val: 0.6698\n",
      "Epoch 2/50, Loss: 1.1048, val: 0.6710\n",
      "Epoch 3/50, Loss: 1.0810, val: 0.6796\n",
      "Epoch 4/50, Loss: 1.0705, val: 0.6756\n",
      "Epoch 5/50, Loss: 1.0567, val: 0.6808\n",
      "Epoch 6/50, Loss: 1.0432, val: 0.6798\n",
      "Epoch 7/50, Loss: 1.0345, val: 0.6829\n",
      "Epoch 8/50, Loss: 1.0254, val: 0.6817\n",
      "Epoch 9/50, Loss: 1.0196, val: 0.6833\n",
      "Epoch 10/50, Loss: 1.0106, val: 0.6825\n",
      "Epoch 11/50, Loss: 1.0092, val: 0.6829\n",
      "Epoch 12/50, Loss: 1.0012, val: 0.6867\n",
      "Epoch 13/50, Loss: 0.9912, val: 0.6856\n",
      "Epoch 14/50, Loss: 0.9875, val: 0.6848\n",
      "Epoch 15/50, Loss: 0.9830, val: 0.6796\n",
      "Epoch 16/50, Loss: 0.9786, val: 0.6850\n",
      "Epoch 17/50, Loss: 0.9705, val: 0.6829\n",
      "Epoch 18/50, Loss: 0.9711, val: 0.6858\n",
      "Epoch 19/50, Loss: 0.9680, val: 0.6810\n",
      "Epoch 20/50, Loss: 0.9637, val: 0.6848\n",
      "Epoch 21/50, Loss: 0.9595, val: 0.6827\n",
      "Epoch 22/50, Loss: 0.9542, val: 0.6835\n",
      "Epoch 23/50, Loss: 0.9532, val: 0.6867\n",
      "Epoch 24/50, Loss: 0.9515, val: 0.6835\n",
      "Epoch 25/50, Loss: 0.9486, val: 0.6802\n",
      "Epoch 26/50, Loss: 0.9490, val: 0.6800\n",
      "Epoch 27/50, Loss: 0.9465, val: 0.6821\n",
      "Epoch 28/50, Loss: 0.9404, val: 0.6829\n",
      "Epoch 29/50, Loss: 0.9414, val: 0.6842\n",
      "Epoch 30/50, Loss: 0.9381, val: 0.6825\n",
      "Epoch 31/50, Loss: 0.9342, val: 0.6844\n",
      "Epoch 32/50, Loss: 0.9345, val: 0.6823\n",
      "Epoch 33/50, Loss: 0.9315, val: 0.6813\n",
      "Epoch 34/50, Loss: 0.9295, val: 0.6823\n",
      "Epoch 35/50, Loss: 0.9291, val: 0.6837\n",
      "Epoch 36/50, Loss: 0.9334, val: 0.6787\n",
      "Epoch 37/50, Loss: 0.9344, val: 0.6831\n",
      "Epoch 38/50, Loss: 0.9260, val: 0.6802\n",
      "Epoch 39/50, Loss: 0.9211, val: 0.6829\n",
      "Epoch 40/50, Loss: 0.9209, val: 0.6842\n",
      "Epoch 41/50, Loss: 0.9254, val: 0.6842\n",
      "Epoch 42/50, Loss: 0.9190, val: 0.6819\n",
      "Epoch 43/50, Loss: 0.9181, val: 0.6825\n",
      "Epoch 44/50, Loss: 0.9143, val: 0.6813\n",
      "Epoch 45/50, Loss: 0.9147, val: 0.6815\n",
      "Epoch 46/50, Loss: 0.9158, val: 0.6792\n",
      "Epoch 47/50, Loss: 0.9188, val: 0.6840\n",
      "Epoch 48/50, Loss: 0.9143, val: 0.6837\n",
      "Epoch 49/50, Loss: 0.9101, val: 0.6785\n",
      "Epoch 50/50, Loss: 0.9109, val: 0.6840\n",
      "Epoch 1/50, Loss: 1.2719, val: 0.6652\n",
      "Epoch 2/50, Loss: 1.1027, val: 0.6723\n",
      "Epoch 3/50, Loss: 1.0831, val: 0.6756\n",
      "Epoch 4/50, Loss: 1.0681, val: 0.6800\n",
      "Epoch 5/50, Loss: 1.0534, val: 0.6706\n",
      "Epoch 6/50, Loss: 1.0457, val: 0.6800\n",
      "Epoch 7/50, Loss: 1.0346, val: 0.6815\n",
      "Epoch 8/50, Loss: 1.0252, val: 0.6831\n",
      "Epoch 9/50, Loss: 1.0149, val: 0.6825\n",
      "Epoch 10/50, Loss: 1.0084, val: 0.6815\n",
      "Epoch 11/50, Loss: 1.0008, val: 0.6813\n",
      "Epoch 12/50, Loss: 0.9979, val: 0.6815\n",
      "Epoch 13/50, Loss: 0.9908, val: 0.6867\n",
      "Epoch 14/50, Loss: 0.9852, val: 0.6854\n",
      "Epoch 15/50, Loss: 0.9791, val: 0.6817\n",
      "Epoch 16/50, Loss: 0.9758, val: 0.6746\n",
      "Epoch 17/50, Loss: 0.9732, val: 0.6844\n",
      "Epoch 18/50, Loss: 0.9665, val: 0.6865\n",
      "Epoch 19/50, Loss: 0.9665, val: 0.6827\n",
      "Epoch 20/50, Loss: 0.9626, val: 0.6810\n",
      "Epoch 21/50, Loss: 0.9638, val: 0.6842\n",
      "Epoch 22/50, Loss: 0.9528, val: 0.6840\n",
      "Epoch 23/50, Loss: 0.9480, val: 0.6846\n",
      "Epoch 24/50, Loss: 0.9494, val: 0.6817\n",
      "Epoch 25/50, Loss: 0.9467, val: 0.6794\n",
      "Epoch 26/50, Loss: 0.9433, val: 0.6827\n",
      "Epoch 27/50, Loss: 0.9447, val: 0.6798\n",
      "Epoch 28/50, Loss: 0.9491, val: 0.6844\n",
      "Epoch 29/50, Loss: 0.9399, val: 0.6825\n",
      "Epoch 30/50, Loss: 0.9370, val: 0.6840\n",
      "Epoch 31/50, Loss: 0.9338, val: 0.6806\n",
      "Epoch 32/50, Loss: 0.9313, val: 0.6817\n",
      "Epoch 33/50, Loss: 0.9306, val: 0.6806\n",
      "Epoch 34/50, Loss: 0.9339, val: 0.6827\n",
      "Epoch 35/50, Loss: 0.9297, val: 0.6844\n",
      "Epoch 36/50, Loss: 0.9242, val: 0.6823\n",
      "Epoch 37/50, Loss: 0.9264, val: 0.6785\n",
      "Epoch 38/50, Loss: 0.9237, val: 0.6817\n",
      "Epoch 39/50, Loss: 0.9308, val: 0.6833\n",
      "Epoch 40/50, Loss: 0.9231, val: 0.6821\n",
      "Epoch 41/50, Loss: 0.9204, val: 0.6840\n",
      "Epoch 42/50, Loss: 0.9177, val: 0.6829\n",
      "Epoch 43/50, Loss: 0.9173, val: 0.6790\n",
      "Epoch 44/50, Loss: 0.9169, val: 0.6825\n",
      "Epoch 45/50, Loss: 0.9185, val: 0.6833\n",
      "Epoch 46/50, Loss: 0.9127, val: 0.6829\n",
      "Epoch 47/50, Loss: 0.9160, val: 0.6813\n",
      "Epoch 48/50, Loss: 0.9194, val: 0.6823\n",
      "Epoch 49/50, Loss: 0.9118, val: 0.6800\n",
      "Epoch 50/50, Loss: 0.9128, val: 0.6835\n",
      "Epoch 1/50, Loss: 1.2899, val: 0.6658\n",
      "Epoch 2/50, Loss: 1.1092, val: 0.6773\n",
      "Epoch 3/50, Loss: 1.0866, val: 0.6748\n",
      "Epoch 4/50, Loss: 1.0704, val: 0.6756\n",
      "Epoch 5/50, Loss: 1.0583, val: 0.6790\n",
      "Epoch 6/50, Loss: 1.0460, val: 0.6840\n",
      "Epoch 7/50, Loss: 1.0355, val: 0.6800\n",
      "Epoch 8/50, Loss: 1.0268, val: 0.6833\n",
      "Epoch 9/50, Loss: 1.0201, val: 0.6804\n",
      "Epoch 10/50, Loss: 1.0115, val: 0.6850\n",
      "Epoch 11/50, Loss: 1.0076, val: 0.6808\n",
      "Epoch 12/50, Loss: 0.9983, val: 0.6831\n",
      "Epoch 13/50, Loss: 0.9952, val: 0.6850\n",
      "Epoch 14/50, Loss: 0.9895, val: 0.6810\n",
      "Epoch 15/50, Loss: 0.9828, val: 0.6831\n",
      "Epoch 16/50, Loss: 0.9769, val: 0.6848\n",
      "Epoch 17/50, Loss: 0.9764, val: 0.6821\n",
      "Epoch 18/50, Loss: 0.9761, val: 0.6827\n",
      "Epoch 19/50, Loss: 0.9662, val: 0.6819\n",
      "Epoch 20/50, Loss: 0.9634, val: 0.6821\n",
      "Epoch 21/50, Loss: 0.9609, val: 0.6819\n",
      "Epoch 22/50, Loss: 0.9582, val: 0.6815\n",
      "Epoch 23/50, Loss: 0.9540, val: 0.6823\n",
      "Epoch 24/50, Loss: 0.9520, val: 0.6810\n",
      "Epoch 25/50, Loss: 0.9500, val: 0.6810\n",
      "Epoch 26/50, Loss: 0.9475, val: 0.6752\n",
      "Epoch 27/50, Loss: 0.9450, val: 0.6837\n",
      "Epoch 28/50, Loss: 0.9399, val: 0.6819\n",
      "Epoch 29/50, Loss: 0.9420, val: 0.6810\n",
      "Epoch 30/50, Loss: 0.9395, val: 0.6817\n",
      "Epoch 31/50, Loss: 0.9333, val: 0.6815\n",
      "Epoch 32/50, Loss: 0.9291, val: 0.6819\n",
      "Epoch 33/50, Loss: 0.9300, val: 0.6806\n",
      "Epoch 34/50, Loss: 0.9260, val: 0.6825\n",
      "Epoch 35/50, Loss: 0.9279, val: 0.6821\n",
      "Epoch 36/50, Loss: 0.9298, val: 0.6810\n",
      "Epoch 37/50, Loss: 0.9309, val: 0.6808\n",
      "Epoch 38/50, Loss: 0.9248, val: 0.6840\n",
      "Epoch 39/50, Loss: 0.9246, val: 0.6787\n",
      "Epoch 40/50, Loss: 0.9226, val: 0.6813\n",
      "Epoch 41/50, Loss: 0.9197, val: 0.6775\n",
      "Epoch 42/50, Loss: 0.9181, val: 0.6794\n",
      "Epoch 43/50, Loss: 0.9165, val: 0.6827\n",
      "Epoch 44/50, Loss: 0.9178, val: 0.6827\n",
      "Epoch 45/50, Loss: 0.9138, val: 0.6794\n",
      "Epoch 46/50, Loss: 0.9300, val: 0.6785\n",
      "Epoch 47/50, Loss: 0.9142, val: 0.6831\n",
      "Epoch 48/50, Loss: 0.9148, val: 0.6769\n",
      "Epoch 49/50, Loss: 0.9090, val: 0.6833\n",
      "Epoch 50/50, Loss: 0.9202, val: 0.6823\n",
      "Epoch 1/50, Loss: 1.2617, val: 0.6717\n",
      "Epoch 2/50, Loss: 1.1030, val: 0.6735\n",
      "Epoch 3/50, Loss: 1.0837, val: 0.6777\n",
      "Epoch 4/50, Loss: 1.0701, val: 0.6721\n",
      "Epoch 5/50, Loss: 1.0586, val: 0.6800\n",
      "Epoch 6/50, Loss: 1.0451, val: 0.6792\n",
      "Epoch 7/50, Loss: 1.0378, val: 0.6787\n",
      "Epoch 8/50, Loss: 1.0282, val: 0.6835\n",
      "Epoch 9/50, Loss: 1.0201, val: 0.6840\n",
      "Epoch 10/50, Loss: 1.0111, val: 0.6833\n",
      "Epoch 11/50, Loss: 1.0042, val: 0.6850\n",
      "Epoch 12/50, Loss: 1.0007, val: 0.6819\n",
      "Epoch 13/50, Loss: 0.9941, val: 0.6860\n",
      "Epoch 14/50, Loss: 0.9855, val: 0.6852\n",
      "Epoch 15/50, Loss: 0.9866, val: 0.6850\n",
      "Epoch 16/50, Loss: 0.9804, val: 0.6810\n",
      "Epoch 17/50, Loss: 0.9735, val: 0.6831\n",
      "Epoch 18/50, Loss: 0.9698, val: 0.6783\n",
      "Epoch 19/50, Loss: 0.9665, val: 0.6821\n",
      "Epoch 20/50, Loss: 0.9647, val: 0.6819\n",
      "Epoch 21/50, Loss: 0.9632, val: 0.6829\n",
      "Epoch 22/50, Loss: 0.9658, val: 0.6833\n",
      "Epoch 23/50, Loss: 0.9596, val: 0.6833\n",
      "Epoch 24/50, Loss: 0.9555, val: 0.6815\n",
      "Epoch 25/50, Loss: 0.9552, val: 0.6842\n",
      "Epoch 26/50, Loss: 0.9473, val: 0.6846\n",
      "Epoch 27/50, Loss: 0.9423, val: 0.6825\n",
      "Epoch 28/50, Loss: 0.9424, val: 0.6823\n",
      "Epoch 29/50, Loss: 0.9399, val: 0.6792\n",
      "Epoch 30/50, Loss: 0.9416, val: 0.6823\n",
      "Epoch 31/50, Loss: 0.9371, val: 0.6775\n",
      "Epoch 32/50, Loss: 0.9344, val: 0.6813\n",
      "Epoch 33/50, Loss: 0.9362, val: 0.6821\n",
      "Epoch 34/50, Loss: 0.9330, val: 0.6827\n",
      "Epoch 35/50, Loss: 0.9290, val: 0.6798\n",
      "Epoch 36/50, Loss: 0.9312, val: 0.6813\n",
      "Epoch 37/50, Loss: 0.9276, val: 0.6798\n",
      "Epoch 38/50, Loss: 0.9251, val: 0.6806\n",
      "Epoch 39/50, Loss: 0.9294, val: 0.6835\n",
      "Epoch 40/50, Loss: 0.9267, val: 0.6815\n",
      "Epoch 41/50, Loss: 0.9251, val: 0.6773\n",
      "Epoch 42/50, Loss: 0.9249, val: 0.6808\n",
      "Epoch 43/50, Loss: 0.9203, val: 0.6821\n",
      "Epoch 44/50, Loss: 0.9223, val: 0.6819\n",
      "Epoch 45/50, Loss: 0.9189, val: 0.6821\n",
      "Epoch 46/50, Loss: 0.9216, val: 0.6821\n",
      "Epoch 47/50, Loss: 0.9230, val: 0.6785\n",
      "Epoch 48/50, Loss: 0.9150, val: 0.6833\n",
      "Epoch 49/50, Loss: 0.9111, val: 0.6827\n",
      "Epoch 50/50, Loss: 0.9123, val: 0.6823\n",
      "Epoch 1/50, Loss: 1.2629, val: 0.6729\n",
      "Epoch 2/50, Loss: 1.1046, val: 0.6756\n",
      "Epoch 3/50, Loss: 1.0792, val: 0.6715\n",
      "Epoch 4/50, Loss: 1.0666, val: 0.6742\n",
      "Epoch 5/50, Loss: 1.0552, val: 0.6796\n",
      "Epoch 6/50, Loss: 1.0459, val: 0.6823\n",
      "Epoch 7/50, Loss: 1.0346, val: 0.6815\n",
      "Epoch 8/50, Loss: 1.0280, val: 0.6779\n",
      "Epoch 9/50, Loss: 1.0182, val: 0.6829\n",
      "Epoch 10/50, Loss: 1.0160, val: 0.6852\n",
      "Epoch 11/50, Loss: 1.0097, val: 0.6835\n",
      "Epoch 12/50, Loss: 1.0037, val: 0.6833\n",
      "Epoch 13/50, Loss: 0.9970, val: 0.6777\n",
      "Epoch 14/50, Loss: 0.9924, val: 0.6808\n",
      "Epoch 15/50, Loss: 0.9862, val: 0.6815\n",
      "Epoch 16/50, Loss: 0.9820, val: 0.6837\n",
      "Epoch 17/50, Loss: 0.9767, val: 0.6823\n",
      "Epoch 18/50, Loss: 0.9721, val: 0.6833\n",
      "Epoch 19/50, Loss: 0.9718, val: 0.6827\n",
      "Epoch 20/50, Loss: 0.9697, val: 0.6823\n",
      "Epoch 21/50, Loss: 0.9635, val: 0.6829\n",
      "Epoch 22/50, Loss: 0.9589, val: 0.6815\n",
      "Epoch 23/50, Loss: 0.9526, val: 0.6831\n",
      "Epoch 24/50, Loss: 0.9535, val: 0.6808\n",
      "Epoch 25/50, Loss: 0.9517, val: 0.6804\n",
      "Epoch 26/50, Loss: 0.9477, val: 0.6842\n",
      "Epoch 27/50, Loss: 0.9494, val: 0.6823\n",
      "Epoch 28/50, Loss: 0.9433, val: 0.6835\n",
      "Epoch 29/50, Loss: 0.9406, val: 0.6858\n",
      "Epoch 30/50, Loss: 0.9368, val: 0.6829\n",
      "Epoch 31/50, Loss: 0.9357, val: 0.6825\n",
      "Epoch 32/50, Loss: 0.9317, val: 0.6815\n",
      "Epoch 33/50, Loss: 0.9387, val: 0.6817\n",
      "Epoch 34/50, Loss: 0.9330, val: 0.6831\n",
      "Epoch 35/50, Loss: 0.9288, val: 0.6837\n",
      "Epoch 36/50, Loss: 0.9312, val: 0.6810\n",
      "Epoch 37/50, Loss: 0.9259, val: 0.6835\n",
      "Epoch 38/50, Loss: 0.9253, val: 0.6846\n",
      "Epoch 39/50, Loss: 0.9252, val: 0.6852\n",
      "Epoch 40/50, Loss: 0.9194, val: 0.6833\n",
      "Epoch 41/50, Loss: 0.9175, val: 0.6846\n",
      "Epoch 42/50, Loss: 0.9175, val: 0.6810\n",
      "Epoch 43/50, Loss: 0.9188, val: 0.6810\n",
      "Epoch 44/50, Loss: 0.9216, val: 0.6875\n",
      "Epoch 45/50, Loss: 0.9203, val: 0.6817\n",
      "Epoch 46/50, Loss: 0.9214, val: 0.6842\n",
      "Epoch 47/50, Loss: 0.9176, val: 0.6819\n",
      "Epoch 48/50, Loss: 0.9136, val: 0.6810\n",
      "Epoch 49/50, Loss: 0.9122, val: 0.6842\n",
      "Epoch 50/50, Loss: 0.9133, val: 0.6831\n",
      "         dataset_name  mean_val_accuracy  mean_accuracy  std_accuracy\n",
      "0  [FashionMNIST0, 3]            0.68159        0.93925      0.002624\n",
      "Epoch 1/50, Loss: 2.0336, val: 0.3771\n",
      "Epoch 2/50, Loss: 2.0146, val: 0.3812\n",
      "Epoch 3/50, Loss: 2.0116, val: 0.3767\n",
      "Epoch 4/50, Loss: 2.0098, val: 0.3738\n",
      "Epoch 5/50, Loss: 2.0085, val: 0.3773\n",
      "Epoch 6/50, Loss: 2.0059, val: 0.3785\n",
      "Epoch 7/50, Loss: 2.0043, val: 0.3756\n",
      "Epoch 8/50, Loss: 2.0020, val: 0.3754\n",
      "Epoch 9/50, Loss: 2.0011, val: 0.3725\n",
      "Epoch 10/50, Loss: 1.9988, val: 0.3765\n",
      "Epoch 11/50, Loss: 1.9972, val: 0.3754\n",
      "Epoch 12/50, Loss: 1.9948, val: 0.3769\n",
      "Epoch 13/50, Loss: 1.9917, val: 0.3771\n",
      "Epoch 14/50, Loss: 1.9904, val: 0.3648\n",
      "Epoch 15/50, Loss: 1.9879, val: 0.3729\n",
      "Epoch 16/50, Loss: 1.9859, val: 0.3708\n",
      "Epoch 17/50, Loss: 1.9827, val: 0.3744\n",
      "Epoch 18/50, Loss: 1.9829, val: 0.3771\n",
      "Epoch 19/50, Loss: 1.9789, val: 0.3721\n",
      "Epoch 20/50, Loss: 1.9773, val: 0.3648\n",
      "Epoch 21/50, Loss: 1.9743, val: 0.3675\n",
      "Epoch 22/50, Loss: 1.9734, val: 0.3721\n",
      "Epoch 23/50, Loss: 1.9693, val: 0.3688\n",
      "Epoch 24/50, Loss: 1.9683, val: 0.3696\n",
      "Epoch 25/50, Loss: 1.9656, val: 0.3706\n",
      "Epoch 26/50, Loss: 1.9638, val: 0.3708\n",
      "Epoch 27/50, Loss: 1.9620, val: 0.3679\n",
      "Epoch 28/50, Loss: 1.9580, val: 0.3652\n",
      "Epoch 29/50, Loss: 1.9571, val: 0.3721\n",
      "Epoch 30/50, Loss: 1.9571, val: 0.3681\n",
      "Epoch 31/50, Loss: 1.9535, val: 0.3648\n",
      "Epoch 32/50, Loss: 1.9508, val: 0.3656\n",
      "Epoch 33/50, Loss: 1.9505, val: 0.3656\n",
      "Epoch 34/50, Loss: 1.9479, val: 0.3690\n",
      "Epoch 35/50, Loss: 1.9462, val: 0.3663\n",
      "Epoch 36/50, Loss: 1.9463, val: 0.3638\n",
      "Epoch 37/50, Loss: 1.9424, val: 0.3660\n",
      "Epoch 38/50, Loss: 1.9407, val: 0.3650\n",
      "Epoch 39/50, Loss: 1.9396, val: 0.3623\n",
      "Epoch 40/50, Loss: 1.9371, val: 0.3669\n",
      "Epoch 41/50, Loss: 1.9371, val: 0.3623\n",
      "Epoch 42/50, Loss: 1.9340, val: 0.3617\n",
      "Epoch 43/50, Loss: 1.9345, val: 0.3627\n",
      "Epoch 44/50, Loss: 1.9307, val: 0.3638\n",
      "Epoch 45/50, Loss: 1.9296, val: 0.3631\n",
      "Epoch 46/50, Loss: 1.9281, val: 0.3610\n",
      "Epoch 47/50, Loss: 1.9303, val: 0.3629\n",
      "Epoch 48/50, Loss: 1.9262, val: 0.3681\n",
      "Epoch 49/50, Loss: 1.9216, val: 0.3585\n",
      "Epoch 50/50, Loss: 1.9236, val: 0.3625\n",
      "Epoch 1/50, Loss: 2.0375, val: 0.3715\n",
      "Epoch 2/50, Loss: 2.0149, val: 0.3819\n",
      "Epoch 3/50, Loss: 2.0123, val: 0.3812\n",
      "Epoch 4/50, Loss: 2.0101, val: 0.3794\n",
      "Epoch 5/50, Loss: 2.0071, val: 0.3802\n",
      "Epoch 6/50, Loss: 2.0063, val: 0.3754\n",
      "Epoch 7/50, Loss: 2.0049, val: 0.3765\n",
      "Epoch 8/50, Loss: 2.0015, val: 0.3729\n",
      "Epoch 9/50, Loss: 2.0006, val: 0.3773\n",
      "Epoch 10/50, Loss: 1.9985, val: 0.3710\n",
      "Epoch 11/50, Loss: 1.9957, val: 0.3777\n",
      "Epoch 12/50, Loss: 1.9950, val: 0.3640\n",
      "Epoch 13/50, Loss: 1.9927, val: 0.3708\n",
      "Epoch 14/50, Loss: 1.9892, val: 0.3756\n",
      "Epoch 15/50, Loss: 1.9889, val: 0.3700\n",
      "Epoch 16/50, Loss: 1.9871, val: 0.3752\n",
      "Epoch 17/50, Loss: 1.9835, val: 0.3715\n",
      "Epoch 18/50, Loss: 1.9810, val: 0.3752\n",
      "Epoch 19/50, Loss: 1.9792, val: 0.3679\n",
      "Epoch 20/50, Loss: 1.9787, val: 0.3688\n",
      "Epoch 21/50, Loss: 1.9756, val: 0.3773\n",
      "Epoch 22/50, Loss: 1.9742, val: 0.3713\n",
      "Epoch 23/50, Loss: 1.9710, val: 0.3717\n",
      "Epoch 24/50, Loss: 1.9687, val: 0.3677\n",
      "Epoch 25/50, Loss: 1.9673, val: 0.3727\n",
      "Epoch 26/50, Loss: 1.9648, val: 0.3733\n",
      "Epoch 27/50, Loss: 1.9618, val: 0.3681\n",
      "Epoch 28/50, Loss: 1.9604, val: 0.3673\n",
      "Epoch 29/50, Loss: 1.9597, val: 0.3704\n",
      "Epoch 30/50, Loss: 1.9569, val: 0.3740\n",
      "Epoch 31/50, Loss: 1.9541, val: 0.3679\n",
      "Epoch 32/50, Loss: 1.9528, val: 0.3702\n",
      "Epoch 33/50, Loss: 1.9508, val: 0.3683\n",
      "Epoch 34/50, Loss: 1.9477, val: 0.3723\n",
      "Epoch 35/50, Loss: 1.9465, val: 0.3696\n",
      "Epoch 36/50, Loss: 1.9447, val: 0.3608\n",
      "Epoch 37/50, Loss: 1.9441, val: 0.3762\n",
      "Epoch 38/50, Loss: 1.9445, val: 0.3631\n",
      "Epoch 39/50, Loss: 1.9403, val: 0.3696\n",
      "Epoch 40/50, Loss: 1.9374, val: 0.3640\n",
      "Epoch 41/50, Loss: 1.9357, val: 0.3683\n",
      "Epoch 42/50, Loss: 1.9348, val: 0.3604\n",
      "Epoch 43/50, Loss: 1.9325, val: 0.3669\n",
      "Epoch 44/50, Loss: 1.9318, val: 0.3669\n",
      "Epoch 45/50, Loss: 1.9295, val: 0.3663\n",
      "Epoch 46/50, Loss: 1.9276, val: 0.3648\n",
      "Epoch 47/50, Loss: 1.9285, val: 0.3613\n",
      "Epoch 48/50, Loss: 1.9256, val: 0.3650\n",
      "Epoch 49/50, Loss: 1.9236, val: 0.3608\n",
      "Epoch 50/50, Loss: 1.9207, val: 0.3631\n",
      "Epoch 1/50, Loss: 2.0359, val: 0.3715\n",
      "Epoch 2/50, Loss: 2.0164, val: 0.3790\n",
      "Epoch 3/50, Loss: 2.0118, val: 0.3806\n",
      "Epoch 4/50, Loss: 2.0095, val: 0.3717\n",
      "Epoch 5/50, Loss: 2.0083, val: 0.3740\n",
      "Epoch 6/50, Loss: 2.0054, val: 0.3731\n",
      "Epoch 7/50, Loss: 2.0037, val: 0.3771\n",
      "Epoch 8/50, Loss: 2.0012, val: 0.3748\n",
      "Epoch 9/50, Loss: 1.9989, val: 0.3740\n",
      "Epoch 10/50, Loss: 1.9975, val: 0.3783\n",
      "Epoch 11/50, Loss: 1.9961, val: 0.3760\n",
      "Epoch 12/50, Loss: 1.9928, val: 0.3698\n",
      "Epoch 13/50, Loss: 1.9909, val: 0.3738\n",
      "Epoch 14/50, Loss: 1.9889, val: 0.3725\n",
      "Epoch 15/50, Loss: 1.9855, val: 0.3694\n",
      "Epoch 16/50, Loss: 1.9830, val: 0.3731\n",
      "Epoch 17/50, Loss: 1.9808, val: 0.3733\n",
      "Epoch 18/50, Loss: 1.9786, val: 0.3719\n",
      "Epoch 19/50, Loss: 1.9778, val: 0.3719\n",
      "Epoch 20/50, Loss: 1.9736, val: 0.3717\n",
      "Epoch 21/50, Loss: 1.9736, val: 0.3677\n",
      "Epoch 22/50, Loss: 1.9695, val: 0.3683\n",
      "Epoch 23/50, Loss: 1.9682, val: 0.3733\n",
      "Epoch 24/50, Loss: 1.9664, val: 0.3644\n",
      "Epoch 25/50, Loss: 1.9654, val: 0.3683\n",
      "Epoch 26/50, Loss: 1.9613, val: 0.3710\n",
      "Epoch 27/50, Loss: 1.9592, val: 0.3688\n",
      "Epoch 28/50, Loss: 1.9599, val: 0.3679\n",
      "Epoch 29/50, Loss: 1.9579, val: 0.3646\n",
      "Epoch 30/50, Loss: 1.9535, val: 0.3702\n",
      "Epoch 31/50, Loss: 1.9513, val: 0.3685\n",
      "Epoch 32/50, Loss: 1.9492, val: 0.3652\n",
      "Epoch 33/50, Loss: 1.9470, val: 0.3694\n",
      "Epoch 34/50, Loss: 1.9460, val: 0.3673\n",
      "Epoch 35/50, Loss: 1.9428, val: 0.3615\n",
      "Epoch 36/50, Loss: 1.9423, val: 0.3663\n",
      "Epoch 37/50, Loss: 1.9398, val: 0.3600\n",
      "Epoch 38/50, Loss: 1.9369, val: 0.3652\n",
      "Epoch 39/50, Loss: 1.9362, val: 0.3660\n",
      "Epoch 40/50, Loss: 1.9351, val: 0.3631\n",
      "Epoch 41/50, Loss: 1.9344, val: 0.3638\n",
      "Epoch 42/50, Loss: 1.9311, val: 0.3623\n",
      "Epoch 43/50, Loss: 1.9296, val: 0.3665\n",
      "Epoch 44/50, Loss: 1.9282, val: 0.3685\n",
      "Epoch 45/50, Loss: 1.9283, val: 0.3640\n",
      "Epoch 46/50, Loss: 1.9251, val: 0.3638\n",
      "Epoch 47/50, Loss: 1.9243, val: 0.3598\n",
      "Epoch 48/50, Loss: 1.9240, val: 0.3583\n",
      "Epoch 49/50, Loss: 1.9206, val: 0.3642\n",
      "Epoch 50/50, Loss: 1.9183, val: 0.3552\n",
      "Epoch 1/50, Loss: 2.0378, val: 0.3721\n",
      "Epoch 2/50, Loss: 2.0154, val: 0.3767\n",
      "Epoch 3/50, Loss: 2.0127, val: 0.3798\n",
      "Epoch 4/50, Loss: 2.0103, val: 0.3777\n",
      "Epoch 5/50, Loss: 2.0085, val: 0.3765\n",
      "Epoch 6/50, Loss: 2.0071, val: 0.3787\n",
      "Epoch 7/50, Loss: 2.0045, val: 0.3792\n",
      "Epoch 8/50, Loss: 2.0025, val: 0.3783\n",
      "Epoch 9/50, Loss: 2.0006, val: 0.3708\n",
      "Epoch 10/50, Loss: 1.9990, val: 0.3773\n",
      "Epoch 11/50, Loss: 1.9970, val: 0.3769\n",
      "Epoch 12/50, Loss: 1.9955, val: 0.3721\n",
      "Epoch 13/50, Loss: 1.9931, val: 0.3748\n",
      "Epoch 14/50, Loss: 1.9898, val: 0.3740\n",
      "Epoch 15/50, Loss: 1.9883, val: 0.3692\n",
      "Epoch 16/50, Loss: 1.9859, val: 0.3746\n",
      "Epoch 17/50, Loss: 1.9847, val: 0.3746\n",
      "Epoch 18/50, Loss: 1.9816, val: 0.3756\n",
      "Epoch 19/50, Loss: 1.9800, val: 0.3758\n",
      "Epoch 20/50, Loss: 1.9770, val: 0.3696\n",
      "Epoch 21/50, Loss: 1.9736, val: 0.3715\n",
      "Epoch 22/50, Loss: 1.9768, val: 0.3723\n",
      "Epoch 23/50, Loss: 1.9716, val: 0.3758\n",
      "Epoch 24/50, Loss: 1.9686, val: 0.3702\n",
      "Epoch 25/50, Loss: 1.9659, val: 0.3727\n",
      "Epoch 26/50, Loss: 1.9665, val: 0.3717\n",
      "Epoch 27/50, Loss: 1.9632, val: 0.3733\n",
      "Epoch 28/50, Loss: 1.9606, val: 0.3727\n",
      "Epoch 29/50, Loss: 1.9588, val: 0.3640\n",
      "Epoch 30/50, Loss: 1.9565, val: 0.3721\n",
      "Epoch 31/50, Loss: 1.9548, val: 0.3610\n",
      "Epoch 32/50, Loss: 1.9535, val: 0.3690\n",
      "Epoch 33/50, Loss: 1.9517, val: 0.3652\n",
      "Epoch 34/50, Loss: 1.9479, val: 0.3717\n",
      "Epoch 35/50, Loss: 1.9463, val: 0.3671\n",
      "Epoch 36/50, Loss: 1.9442, val: 0.3648\n",
      "Epoch 37/50, Loss: 1.9444, val: 0.3681\n",
      "Epoch 38/50, Loss: 1.9410, val: 0.3708\n",
      "Epoch 39/50, Loss: 1.9402, val: 0.3679\n",
      "Epoch 40/50, Loss: 1.9396, val: 0.3690\n",
      "Epoch 41/50, Loss: 1.9373, val: 0.3623\n",
      "Epoch 42/50, Loss: 1.9362, val: 0.3685\n",
      "Epoch 43/50, Loss: 1.9352, val: 0.3660\n",
      "Epoch 44/50, Loss: 1.9337, val: 0.3667\n",
      "Epoch 45/50, Loss: 1.9308, val: 0.3685\n",
      "Epoch 46/50, Loss: 1.9295, val: 0.3683\n",
      "Epoch 47/50, Loss: 1.9303, val: 0.3631\n",
      "Epoch 48/50, Loss: 1.9263, val: 0.3671\n",
      "Epoch 49/50, Loss: 1.9245, val: 0.3710\n",
      "Epoch 50/50, Loss: 1.9279, val: 0.3688\n",
      "Epoch 1/50, Loss: 2.0478, val: 0.3350\n",
      "Epoch 2/50, Loss: 2.0310, val: 0.3415\n",
      "Epoch 3/50, Loss: 2.0297, val: 0.3452\n",
      "Epoch 4/50, Loss: 2.0277, val: 0.3425\n",
      "Epoch 5/50, Loss: 2.0267, val: 0.3381\n",
      "Epoch 6/50, Loss: 2.0252, val: 0.3440\n",
      "Epoch 7/50, Loss: 2.0245, val: 0.3427\n",
      "Epoch 8/50, Loss: 2.0237, val: 0.3367\n",
      "Epoch 9/50, Loss: 2.0225, val: 0.3448\n",
      "Epoch 10/50, Loss: 2.0219, val: 0.3448\n",
      "Epoch 11/50, Loss: 2.0087, val: 0.3702\n",
      "Epoch 12/50, Loss: 2.0047, val: 0.3762\n",
      "Epoch 13/50, Loss: 2.0004, val: 0.3729\n",
      "Epoch 14/50, Loss: 2.0002, val: 0.3735\n",
      "Epoch 15/50, Loss: 1.9971, val: 0.3713\n",
      "Epoch 16/50, Loss: 1.9977, val: 0.3704\n",
      "Epoch 17/50, Loss: 1.9934, val: 0.3744\n",
      "Epoch 18/50, Loss: 1.9918, val: 0.3706\n",
      "Epoch 19/50, Loss: 1.9898, val: 0.3717\n",
      "Epoch 20/50, Loss: 1.9884, val: 0.3767\n",
      "Epoch 21/50, Loss: 1.9855, val: 0.3690\n",
      "Epoch 22/50, Loss: 1.9838, val: 0.3721\n",
      "Epoch 23/50, Loss: 1.9817, val: 0.3677\n",
      "Epoch 24/50, Loss: 1.9803, val: 0.3702\n",
      "Epoch 25/50, Loss: 1.9774, val: 0.3610\n",
      "Epoch 26/50, Loss: 1.9773, val: 0.3710\n",
      "Epoch 27/50, Loss: 1.9743, val: 0.3688\n",
      "Epoch 28/50, Loss: 1.9731, val: 0.3723\n",
      "Epoch 29/50, Loss: 1.9708, val: 0.3683\n",
      "Epoch 30/50, Loss: 1.9691, val: 0.3696\n",
      "Epoch 31/50, Loss: 1.9675, val: 0.3652\n",
      "Epoch 32/50, Loss: 1.9652, val: 0.3694\n",
      "Epoch 33/50, Loss: 1.9639, val: 0.3648\n",
      "Epoch 34/50, Loss: 1.9625, val: 0.3698\n",
      "Epoch 35/50, Loss: 1.9601, val: 0.3677\n",
      "Epoch 36/50, Loss: 1.9575, val: 0.3638\n",
      "Epoch 37/50, Loss: 1.9563, val: 0.3669\n",
      "Epoch 38/50, Loss: 1.9560, val: 0.3631\n",
      "Epoch 39/50, Loss: 1.9537, val: 0.3681\n",
      "Epoch 40/50, Loss: 1.9512, val: 0.3673\n",
      "Epoch 41/50, Loss: 1.9506, val: 0.3646\n",
      "Epoch 42/50, Loss: 1.9493, val: 0.3629\n",
      "Epoch 43/50, Loss: 1.9463, val: 0.3638\n",
      "Epoch 44/50, Loss: 1.9454, val: 0.3648\n",
      "Epoch 45/50, Loss: 1.9453, val: 0.3623\n",
      "Epoch 46/50, Loss: 1.9426, val: 0.3617\n",
      "Epoch 47/50, Loss: 1.9411, val: 0.3631\n",
      "Epoch 48/50, Loss: 1.9380, val: 0.3621\n",
      "Epoch 49/50, Loss: 1.9375, val: 0.3602\n",
      "Epoch 50/50, Loss: 1.9365, val: 0.3583\n",
      "Epoch 1/50, Loss: 2.0346, val: 0.3752\n",
      "Epoch 2/50, Loss: 2.0145, val: 0.3773\n",
      "Epoch 3/50, Loss: 2.0120, val: 0.3742\n",
      "Epoch 4/50, Loss: 2.0095, val: 0.3794\n",
      "Epoch 5/50, Loss: 2.0077, val: 0.3765\n",
      "Epoch 6/50, Loss: 2.0063, val: 0.3719\n",
      "Epoch 7/50, Loss: 2.0048, val: 0.3777\n",
      "Epoch 8/50, Loss: 2.0017, val: 0.3771\n",
      "Epoch 9/50, Loss: 2.0005, val: 0.3738\n",
      "Epoch 10/50, Loss: 1.9994, val: 0.3740\n",
      "Epoch 11/50, Loss: 1.9969, val: 0.3760\n",
      "Epoch 12/50, Loss: 1.9943, val: 0.3710\n",
      "Epoch 13/50, Loss: 1.9922, val: 0.3692\n",
      "Epoch 14/50, Loss: 1.9911, val: 0.3702\n",
      "Epoch 15/50, Loss: 1.9879, val: 0.3721\n",
      "Epoch 16/50, Loss: 1.9859, val: 0.3708\n",
      "Epoch 17/50, Loss: 1.9827, val: 0.3713\n",
      "Epoch 18/50, Loss: 1.9818, val: 0.3685\n",
      "Epoch 19/50, Loss: 1.9805, val: 0.3760\n",
      "Epoch 20/50, Loss: 1.9802, val: 0.3690\n",
      "Epoch 21/50, Loss: 1.9750, val: 0.3702\n",
      "Epoch 22/50, Loss: 1.9733, val: 0.3683\n",
      "Epoch 23/50, Loss: 1.9738, val: 0.3719\n",
      "Epoch 24/50, Loss: 1.9700, val: 0.3738\n",
      "Epoch 25/50, Loss: 1.9669, val: 0.3658\n",
      "Epoch 26/50, Loss: 1.9645, val: 0.3700\n",
      "Epoch 27/50, Loss: 1.9635, val: 0.3606\n",
      "Epoch 28/50, Loss: 1.9620, val: 0.3713\n",
      "Epoch 29/50, Loss: 1.9602, val: 0.3617\n",
      "Epoch 30/50, Loss: 1.9595, val: 0.3729\n",
      "Epoch 31/50, Loss: 1.9567, val: 0.3617\n",
      "Epoch 32/50, Loss: 1.9554, val: 0.3604\n",
      "Epoch 33/50, Loss: 1.9530, val: 0.3683\n",
      "Epoch 34/50, Loss: 1.9528, val: 0.3692\n",
      "Epoch 35/50, Loss: 1.9499, val: 0.3677\n",
      "Epoch 36/50, Loss: 1.9511, val: 0.3671\n",
      "Epoch 37/50, Loss: 1.9453, val: 0.3719\n",
      "Epoch 38/50, Loss: 1.9442, val: 0.3654\n",
      "Epoch 39/50, Loss: 1.9434, val: 0.3635\n",
      "Epoch 40/50, Loss: 1.9438, val: 0.3683\n",
      "Epoch 41/50, Loss: 1.9423, val: 0.3663\n",
      "Epoch 42/50, Loss: 1.9398, val: 0.3565\n",
      "Epoch 43/50, Loss: 1.9385, val: 0.3617\n",
      "Epoch 44/50, Loss: 1.9374, val: 0.3654\n",
      "Epoch 45/50, Loss: 1.9356, val: 0.3640\n",
      "Epoch 46/50, Loss: 1.9330, val: 0.3635\n",
      "Epoch 47/50, Loss: 1.9325, val: 0.3648\n",
      "Epoch 48/50, Loss: 1.9324, val: 0.3633\n",
      "Epoch 49/50, Loss: 1.9305, val: 0.3656\n",
      "Epoch 50/50, Loss: 1.9289, val: 0.3598\n",
      "Epoch 1/50, Loss: 2.0419, val: 0.3719\n",
      "Epoch 2/50, Loss: 2.0172, val: 0.3781\n",
      "Epoch 3/50, Loss: 2.0136, val: 0.3771\n",
      "Epoch 4/50, Loss: 2.0103, val: 0.3727\n",
      "Epoch 5/50, Loss: 2.0082, val: 0.3815\n",
      "Epoch 6/50, Loss: 2.0067, val: 0.3756\n",
      "Epoch 7/50, Loss: 2.0038, val: 0.3738\n",
      "Epoch 8/50, Loss: 2.0026, val: 0.3731\n",
      "Epoch 9/50, Loss: 1.9990, val: 0.3704\n",
      "Epoch 10/50, Loss: 1.9983, val: 0.3769\n",
      "Epoch 11/50, Loss: 1.9965, val: 0.3735\n",
      "Epoch 12/50, Loss: 1.9936, val: 0.3708\n",
      "Epoch 13/50, Loss: 1.9911, val: 0.3738\n",
      "Epoch 14/50, Loss: 1.9896, val: 0.3779\n",
      "Epoch 15/50, Loss: 1.9890, val: 0.3748\n",
      "Epoch 16/50, Loss: 1.9861, val: 0.3758\n",
      "Epoch 17/50, Loss: 1.9846, val: 0.3762\n",
      "Epoch 18/50, Loss: 1.9818, val: 0.3708\n",
      "Epoch 19/50, Loss: 1.9807, val: 0.3723\n",
      "Epoch 20/50, Loss: 1.9776, val: 0.3675\n",
      "Epoch 21/50, Loss: 1.9756, val: 0.3696\n",
      "Epoch 22/50, Loss: 1.9726, val: 0.3700\n",
      "Epoch 23/50, Loss: 1.9705, val: 0.3713\n",
      "Epoch 24/50, Loss: 1.9707, val: 0.3660\n",
      "Epoch 25/50, Loss: 1.9682, val: 0.3685\n",
      "Epoch 26/50, Loss: 1.9662, val: 0.3729\n",
      "Epoch 27/50, Loss: 1.9650, val: 0.3727\n",
      "Epoch 28/50, Loss: 1.9618, val: 0.3710\n",
      "Epoch 29/50, Loss: 1.9611, val: 0.3692\n",
      "Epoch 30/50, Loss: 1.9586, val: 0.3702\n",
      "Epoch 31/50, Loss: 1.9558, val: 0.3640\n",
      "Epoch 32/50, Loss: 1.9544, val: 0.3654\n",
      "Epoch 33/50, Loss: 1.9554, val: 0.3723\n",
      "Epoch 34/50, Loss: 1.9521, val: 0.3677\n",
      "Epoch 35/50, Loss: 1.9495, val: 0.3698\n",
      "Epoch 36/50, Loss: 1.9506, val: 0.3685\n",
      "Epoch 37/50, Loss: 1.9459, val: 0.3700\n",
      "Epoch 38/50, Loss: 1.9463, val: 0.3663\n",
      "Epoch 39/50, Loss: 1.9422, val: 0.3688\n",
      "Epoch 40/50, Loss: 1.9414, val: 0.3675\n",
      "Epoch 41/50, Loss: 1.9412, val: 0.3633\n",
      "Epoch 42/50, Loss: 1.9389, val: 0.3596\n",
      "Epoch 43/50, Loss: 1.9352, val: 0.3625\n",
      "Epoch 44/50, Loss: 1.9340, val: 0.3690\n",
      "Epoch 45/50, Loss: 1.9338, val: 0.3638\n",
      "Epoch 46/50, Loss: 1.9335, val: 0.3619\n",
      "Epoch 47/50, Loss: 1.9298, val: 0.3654\n",
      "Epoch 48/50, Loss: 1.9287, val: 0.3627\n",
      "Epoch 49/50, Loss: 1.9250, val: 0.3658\n",
      "Epoch 50/50, Loss: 1.9256, val: 0.3660\n",
      "Epoch 1/50, Loss: 2.0339, val: 0.3767\n",
      "Epoch 2/50, Loss: 2.0150, val: 0.3831\n",
      "Epoch 3/50, Loss: 2.0121, val: 0.3792\n",
      "Epoch 4/50, Loss: 2.0101, val: 0.3775\n",
      "Epoch 5/50, Loss: 2.0084, val: 0.3787\n",
      "Epoch 6/50, Loss: 2.0068, val: 0.3796\n",
      "Epoch 7/50, Loss: 2.0038, val: 0.3762\n",
      "Epoch 8/50, Loss: 2.0018, val: 0.3702\n",
      "Epoch 9/50, Loss: 2.0004, val: 0.3756\n",
      "Epoch 10/50, Loss: 1.9989, val: 0.3694\n",
      "Epoch 11/50, Loss: 1.9969, val: 0.3781\n",
      "Epoch 12/50, Loss: 1.9937, val: 0.3742\n",
      "Epoch 13/50, Loss: 1.9930, val: 0.3702\n",
      "Epoch 14/50, Loss: 1.9899, val: 0.3750\n",
      "Epoch 15/50, Loss: 1.9880, val: 0.3721\n",
      "Epoch 16/50, Loss: 1.9873, val: 0.3752\n",
      "Epoch 17/50, Loss: 1.9857, val: 0.3748\n",
      "Epoch 18/50, Loss: 1.9820, val: 0.3710\n",
      "Epoch 19/50, Loss: 1.9809, val: 0.3710\n",
      "Epoch 20/50, Loss: 1.9775, val: 0.3733\n",
      "Epoch 21/50, Loss: 1.9762, val: 0.3729\n",
      "Epoch 22/50, Loss: 1.9728, val: 0.3673\n",
      "Epoch 23/50, Loss: 1.9715, val: 0.3708\n",
      "Epoch 24/50, Loss: 1.9707, val: 0.3627\n",
      "Epoch 25/50, Loss: 1.9675, val: 0.3658\n",
      "Epoch 26/50, Loss: 1.9665, val: 0.3704\n",
      "Epoch 27/50, Loss: 1.9625, val: 0.3715\n",
      "Epoch 28/50, Loss: 1.9619, val: 0.3675\n",
      "Epoch 29/50, Loss: 1.9587, val: 0.3663\n",
      "Epoch 30/50, Loss: 1.9572, val: 0.3675\n",
      "Epoch 31/50, Loss: 1.9549, val: 0.3681\n",
      "Epoch 32/50, Loss: 1.9537, val: 0.3652\n",
      "Epoch 33/50, Loss: 1.9521, val: 0.3667\n",
      "Epoch 34/50, Loss: 1.9509, val: 0.3667\n",
      "Epoch 35/50, Loss: 1.9480, val: 0.3660\n",
      "Epoch 36/50, Loss: 1.9480, val: 0.3683\n",
      "Epoch 37/50, Loss: 1.9443, val: 0.3631\n",
      "Epoch 38/50, Loss: 1.9419, val: 0.3706\n",
      "Epoch 39/50, Loss: 1.9388, val: 0.3669\n",
      "Epoch 40/50, Loss: 1.9393, val: 0.3702\n",
      "Epoch 41/50, Loss: 1.9378, val: 0.3725\n",
      "Epoch 42/50, Loss: 1.9353, val: 0.3692\n",
      "Epoch 43/50, Loss: 1.9334, val: 0.3596\n",
      "Epoch 44/50, Loss: 1.9322, val: 0.3675\n",
      "Epoch 45/50, Loss: 1.9322, val: 0.3696\n",
      "Epoch 46/50, Loss: 1.9306, val: 0.3650\n",
      "Epoch 47/50, Loss: 1.9313, val: 0.3650\n",
      "Epoch 48/50, Loss: 1.9289, val: 0.3690\n",
      "Epoch 49/50, Loss: 1.9263, val: 0.3608\n",
      "Epoch 50/50, Loss: 1.9268, val: 0.3654\n",
      "Epoch 1/50, Loss: 2.0333, val: 0.3785\n",
      "Epoch 2/50, Loss: 2.0147, val: 0.3746\n",
      "Epoch 3/50, Loss: 2.0118, val: 0.3815\n",
      "Epoch 4/50, Loss: 2.0091, val: 0.3748\n",
      "Epoch 5/50, Loss: 2.0078, val: 0.3729\n",
      "Epoch 6/50, Loss: 2.0068, val: 0.3779\n",
      "Epoch 7/50, Loss: 2.0043, val: 0.3740\n",
      "Epoch 8/50, Loss: 2.0024, val: 0.3754\n",
      "Epoch 9/50, Loss: 2.0008, val: 0.3762\n",
      "Epoch 10/50, Loss: 1.9990, val: 0.3773\n",
      "Epoch 11/50, Loss: 1.9955, val: 0.3677\n",
      "Epoch 12/50, Loss: 1.9938, val: 0.3758\n",
      "Epoch 13/50, Loss: 1.9920, val: 0.3713\n",
      "Epoch 14/50, Loss: 1.9925, val: 0.3750\n",
      "Epoch 15/50, Loss: 1.9881, val: 0.3756\n",
      "Epoch 16/50, Loss: 1.9855, val: 0.3690\n",
      "Epoch 17/50, Loss: 1.9865, val: 0.3756\n",
      "Epoch 18/50, Loss: 1.9841, val: 0.3735\n",
      "Epoch 19/50, Loss: 1.9804, val: 0.3710\n",
      "Epoch 20/50, Loss: 1.9767, val: 0.3694\n",
      "Epoch 21/50, Loss: 1.9755, val: 0.3681\n",
      "Epoch 22/50, Loss: 1.9744, val: 0.3665\n",
      "Epoch 23/50, Loss: 1.9714, val: 0.3719\n",
      "Epoch 24/50, Loss: 1.9699, val: 0.3717\n",
      "Epoch 25/50, Loss: 1.9658, val: 0.3702\n",
      "Epoch 26/50, Loss: 1.9638, val: 0.3700\n",
      "Epoch 27/50, Loss: 1.9618, val: 0.3650\n",
      "Epoch 28/50, Loss: 1.9613, val: 0.3675\n",
      "Epoch 29/50, Loss: 1.9572, val: 0.3710\n",
      "Epoch 30/50, Loss: 1.9584, val: 0.3688\n",
      "Epoch 31/50, Loss: 1.9536, val: 0.3744\n",
      "Epoch 32/50, Loss: 1.9513, val: 0.3615\n",
      "Epoch 33/50, Loss: 1.9498, val: 0.3733\n",
      "Epoch 34/50, Loss: 1.9485, val: 0.3594\n",
      "Epoch 35/50, Loss: 1.9478, val: 0.3675\n",
      "Epoch 36/50, Loss: 1.9449, val: 0.3685\n",
      "Epoch 37/50, Loss: 1.9444, val: 0.3673\n",
      "Epoch 38/50, Loss: 1.9433, val: 0.3642\n",
      "Epoch 39/50, Loss: 1.9409, val: 0.3660\n",
      "Epoch 40/50, Loss: 1.9394, val: 0.3660\n",
      "Epoch 41/50, Loss: 1.9363, val: 0.3635\n",
      "Epoch 42/50, Loss: 1.9373, val: 0.3715\n",
      "Epoch 43/50, Loss: 1.9347, val: 0.3658\n",
      "Epoch 44/50, Loss: 1.9358, val: 0.3629\n",
      "Epoch 45/50, Loss: 1.9317, val: 0.3677\n",
      "Epoch 46/50, Loss: 1.9293, val: 0.3633\n",
      "Epoch 47/50, Loss: 1.9304, val: 0.3667\n",
      "Epoch 48/50, Loss: 1.9298, val: 0.3656\n",
      "Epoch 49/50, Loss: 1.9240, val: 0.3710\n",
      "Epoch 50/50, Loss: 1.9246, val: 0.3646\n",
      "Epoch 1/50, Loss: 2.0331, val: 0.3752\n",
      "Epoch 2/50, Loss: 2.0148, val: 0.3746\n",
      "Epoch 3/50, Loss: 2.0121, val: 0.3802\n",
      "Epoch 4/50, Loss: 2.0097, val: 0.3775\n",
      "Epoch 5/50, Loss: 2.0084, val: 0.3819\n",
      "Epoch 6/50, Loss: 2.0061, val: 0.3767\n",
      "Epoch 7/50, Loss: 2.0043, val: 0.3754\n",
      "Epoch 8/50, Loss: 2.0035, val: 0.3796\n",
      "Epoch 9/50, Loss: 2.0021, val: 0.3783\n",
      "Epoch 10/50, Loss: 2.0002, val: 0.3769\n",
      "Epoch 11/50, Loss: 1.9966, val: 0.3767\n",
      "Epoch 12/50, Loss: 1.9957, val: 0.3806\n",
      "Epoch 13/50, Loss: 1.9934, val: 0.3769\n",
      "Epoch 14/50, Loss: 1.9908, val: 0.3710\n",
      "Epoch 15/50, Loss: 1.9894, val: 0.3658\n",
      "Epoch 16/50, Loss: 1.9868, val: 0.3727\n",
      "Epoch 17/50, Loss: 1.9843, val: 0.3721\n",
      "Epoch 18/50, Loss: 1.9822, val: 0.3652\n",
      "Epoch 19/50, Loss: 1.9797, val: 0.3696\n",
      "Epoch 20/50, Loss: 1.9772, val: 0.3733\n",
      "Epoch 21/50, Loss: 1.9763, val: 0.3640\n",
      "Epoch 22/50, Loss: 1.9749, val: 0.3719\n",
      "Epoch 23/50, Loss: 1.9729, val: 0.3696\n",
      "Epoch 24/50, Loss: 1.9711, val: 0.3754\n",
      "Epoch 25/50, Loss: 1.9676, val: 0.3673\n",
      "Epoch 26/50, Loss: 1.9659, val: 0.3696\n",
      "Epoch 27/50, Loss: 1.9643, val: 0.3725\n",
      "Epoch 28/50, Loss: 1.9616, val: 0.3683\n",
      "Epoch 29/50, Loss: 1.9607, val: 0.3658\n",
      "Epoch 30/50, Loss: 1.9573, val: 0.3654\n",
      "Epoch 31/50, Loss: 1.9582, val: 0.3704\n",
      "Epoch 32/50, Loss: 1.9547, val: 0.3648\n",
      "Epoch 33/50, Loss: 1.9546, val: 0.3625\n",
      "Epoch 34/50, Loss: 1.9502, val: 0.3619\n",
      "Epoch 35/50, Loss: 1.9488, val: 0.3675\n",
      "Epoch 36/50, Loss: 1.9473, val: 0.3690\n",
      "Epoch 37/50, Loss: 1.9452, val: 0.3710\n",
      "Epoch 38/50, Loss: 1.9446, val: 0.3610\n",
      "Epoch 39/50, Loss: 1.9418, val: 0.3663\n",
      "Epoch 40/50, Loss: 1.9408, val: 0.3656\n",
      "Epoch 41/50, Loss: 1.9387, val: 0.3681\n",
      "Epoch 42/50, Loss: 1.9371, val: 0.3633\n",
      "Epoch 43/50, Loss: 1.9381, val: 0.3627\n",
      "Epoch 44/50, Loss: 1.9339, val: 0.3650\n",
      "Epoch 45/50, Loss: 1.9323, val: 0.3625\n",
      "Epoch 46/50, Loss: 1.9310, val: 0.3644\n",
      "Epoch 47/50, Loss: 1.9300, val: 0.3625\n",
      "Epoch 48/50, Loss: 1.9290, val: 0.3650\n",
      "Epoch 49/50, Loss: 1.9269, val: 0.3613\n",
      "Epoch 50/50, Loss: 1.9289, val: 0.3667\n",
      "         dataset_name  mean_val_accuracy  mean_accuracy  std_accuracy\n",
      "0  [FashionMNIST0, 6]           0.369191       0.818275      0.012256\n",
      "Epoch 1/50, Loss: 1.5189, val: 0.5190\n",
      "Epoch 2/50, Loss: 1.3510, val: 0.5695\n",
      "Epoch 3/50, Loss: 1.2668, val: 0.5867\n",
      "Epoch 4/50, Loss: 1.2223, val: 0.6065\n",
      "Epoch 5/50, Loss: 1.1697, val: 0.6060\n",
      "Epoch 6/50, Loss: 1.1358, val: 0.6085\n",
      "Epoch 7/50, Loss: 1.1038, val: 0.6150\n",
      "Epoch 8/50, Loss: 1.0739, val: 0.6150\n",
      "Epoch 9/50, Loss: 1.0423, val: 0.6112\n",
      "Epoch 10/50, Loss: 1.0168, val: 0.6198\n",
      "Epoch 11/50, Loss: 0.9870, val: 0.6158\n",
      "Epoch 12/50, Loss: 0.9703, val: 0.6122\n",
      "Epoch 13/50, Loss: 0.9470, val: 0.6130\n",
      "Epoch 14/50, Loss: 0.9338, val: 0.6140\n",
      "Epoch 15/50, Loss: 0.9251, val: 0.6238\n",
      "Epoch 16/50, Loss: 0.8731, val: 0.6242\n",
      "Epoch 17/50, Loss: 0.8585, val: 0.6278\n",
      "Epoch 18/50, Loss: 0.8553, val: 0.6115\n",
      "Epoch 19/50, Loss: 0.8342, val: 0.6125\n",
      "Epoch 20/50, Loss: 0.8312, val: 0.6185\n",
      "Epoch 21/50, Loss: 0.8199, val: 0.6120\n",
      "Epoch 22/50, Loss: 0.7990, val: 0.6220\n",
      "Epoch 23/50, Loss: 0.7952, val: 0.6245\n",
      "Epoch 24/50, Loss: 0.7755, val: 0.6280\n",
      "Epoch 25/50, Loss: 0.7638, val: 0.6060\n",
      "Epoch 26/50, Loss: 0.7930, val: 0.6228\n",
      "Epoch 27/50, Loss: 0.7501, val: 0.6192\n",
      "Epoch 28/50, Loss: 0.7509, val: 0.6188\n",
      "Epoch 29/50, Loss: 0.7441, val: 0.6175\n",
      "Epoch 30/50, Loss: 0.7433, val: 0.6105\n",
      "Epoch 31/50, Loss: 0.7438, val: 0.6125\n",
      "Epoch 32/50, Loss: 0.7392, val: 0.6160\n",
      "Epoch 33/50, Loss: 0.7312, val: 0.6155\n",
      "Epoch 34/50, Loss: 0.7105, val: 0.6175\n",
      "Epoch 35/50, Loss: 0.7016, val: 0.6230\n",
      "Epoch 36/50, Loss: 0.7036, val: 0.6222\n",
      "Epoch 37/50, Loss: 0.6951, val: 0.6068\n",
      "Epoch 38/50, Loss: 0.7159, val: 0.6198\n",
      "Epoch 39/50, Loss: 0.7171, val: 0.6118\n",
      "Epoch 40/50, Loss: 0.7018, val: 0.6168\n",
      "Epoch 41/50, Loss: 0.6814, val: 0.6172\n",
      "Epoch 42/50, Loss: 0.6778, val: 0.6135\n",
      "Epoch 43/50, Loss: 0.6674, val: 0.6112\n",
      "Epoch 44/50, Loss: 0.7026, val: 0.6122\n",
      "Epoch 45/50, Loss: 0.7137, val: 0.6268\n",
      "Epoch 46/50, Loss: 0.7057, val: 0.6182\n",
      "Epoch 47/50, Loss: 0.6786, val: 0.6082\n",
      "Epoch 48/50, Loss: 0.6747, val: 0.6180\n",
      "Epoch 49/50, Loss: 0.6836, val: 0.6195\n",
      "Epoch 50/50, Loss: 0.6636, val: 0.6092\n",
      "Epoch 1/50, Loss: 1.5153, val: 0.5180\n",
      "Epoch 2/50, Loss: 1.3461, val: 0.5770\n",
      "Epoch 3/50, Loss: 1.2564, val: 0.5968\n",
      "Epoch 4/50, Loss: 1.2090, val: 0.6082\n",
      "Epoch 5/50, Loss: 1.1598, val: 0.6142\n",
      "Epoch 6/50, Loss: 1.1112, val: 0.6182\n",
      "Epoch 7/50, Loss: 1.0742, val: 0.6200\n",
      "Epoch 8/50, Loss: 1.0680, val: 0.6230\n",
      "Epoch 9/50, Loss: 1.0215, val: 0.6128\n",
      "Epoch 10/50, Loss: 0.9822, val: 0.6075\n",
      "Epoch 11/50, Loss: 0.9683, val: 0.6275\n",
      "Epoch 12/50, Loss: 0.9457, val: 0.6190\n",
      "Epoch 13/50, Loss: 0.9150, val: 0.6100\n",
      "Epoch 14/50, Loss: 0.9025, val: 0.6070\n",
      "Epoch 15/50, Loss: 0.8803, val: 0.6162\n",
      "Epoch 16/50, Loss: 0.8630, val: 0.6110\n",
      "Epoch 17/50, Loss: 0.8541, val: 0.6062\n",
      "Epoch 18/50, Loss: 0.8489, val: 0.6140\n",
      "Epoch 19/50, Loss: 0.8367, val: 0.6160\n",
      "Epoch 20/50, Loss: 0.8066, val: 0.6185\n",
      "Epoch 21/50, Loss: 0.7826, val: 0.6195\n",
      "Epoch 22/50, Loss: 0.7812, val: 0.6212\n",
      "Epoch 23/50, Loss: 0.7791, val: 0.6188\n",
      "Epoch 24/50, Loss: 0.7508, val: 0.6190\n",
      "Epoch 25/50, Loss: 0.7522, val: 0.6182\n",
      "Epoch 26/50, Loss: 0.7531, val: 0.6095\n",
      "Epoch 27/50, Loss: 0.7600, val: 0.6225\n",
      "Epoch 28/50, Loss: 0.7303, val: 0.6150\n",
      "Epoch 29/50, Loss: 0.7517, val: 0.6060\n",
      "Epoch 30/50, Loss: 0.7314, val: 0.6195\n",
      "Epoch 31/50, Loss: 0.7496, val: 0.6160\n",
      "Epoch 32/50, Loss: 0.7223, val: 0.6145\n",
      "Epoch 33/50, Loss: 0.7001, val: 0.6132\n",
      "Epoch 34/50, Loss: 0.7026, val: 0.6232\n",
      "Epoch 35/50, Loss: 0.6847, val: 0.6172\n",
      "Epoch 36/50, Loss: 0.7008, val: 0.6145\n",
      "Epoch 37/50, Loss: 0.7300, val: 0.6082\n",
      "Epoch 38/50, Loss: 0.7055, val: 0.6040\n",
      "Epoch 39/50, Loss: 0.6885, val: 0.6140\n",
      "Epoch 40/50, Loss: 0.7151, val: 0.6132\n",
      "Epoch 41/50, Loss: 0.6887, val: 0.6120\n",
      "Epoch 42/50, Loss: 0.6841, val: 0.6040\n",
      "Epoch 43/50, Loss: 0.6937, val: 0.6082\n",
      "Epoch 44/50, Loss: 0.6855, val: 0.6072\n",
      "Epoch 45/50, Loss: 0.6873, val: 0.6182\n",
      "Epoch 46/50, Loss: 0.6950, val: 0.6040\n",
      "Epoch 47/50, Loss: 0.6951, val: 0.6195\n",
      "Epoch 48/50, Loss: 0.6844, val: 0.6185\n",
      "Epoch 49/50, Loss: 0.6631, val: 0.6132\n",
      "Epoch 50/50, Loss: 0.6799, val: 0.6148\n",
      "Epoch 1/50, Loss: 1.5225, val: 0.5075\n",
      "Epoch 2/50, Loss: 1.3407, val: 0.5810\n",
      "Epoch 3/50, Loss: 1.2573, val: 0.5962\n",
      "Epoch 4/50, Loss: 1.1969, val: 0.6102\n",
      "Epoch 5/50, Loss: 1.1607, val: 0.6040\n",
      "Epoch 6/50, Loss: 1.1213, val: 0.6095\n",
      "Epoch 7/50, Loss: 1.0824, val: 0.6038\n",
      "Epoch 8/50, Loss: 1.0527, val: 0.6058\n",
      "Epoch 9/50, Loss: 1.0202, val: 0.6122\n",
      "Epoch 10/50, Loss: 0.9855, val: 0.6130\n",
      "Epoch 11/50, Loss: 0.9676, val: 0.6200\n",
      "Epoch 12/50, Loss: 0.9363, val: 0.6110\n",
      "Epoch 13/50, Loss: 0.9151, val: 0.6065\n",
      "Epoch 14/50, Loss: 0.9090, val: 0.6108\n",
      "Epoch 15/50, Loss: 0.8853, val: 0.6095\n",
      "Epoch 16/50, Loss: 0.8702, val: 0.6158\n",
      "Epoch 17/50, Loss: 0.8435, val: 0.6135\n",
      "Epoch 18/50, Loss: 0.8331, val: 0.6175\n",
      "Epoch 19/50, Loss: 0.8250, val: 0.6098\n",
      "Epoch 20/50, Loss: 0.8045, val: 0.6175\n",
      "Epoch 21/50, Loss: 0.8035, val: 0.6248\n",
      "Epoch 22/50, Loss: 0.7841, val: 0.6155\n",
      "Epoch 23/50, Loss: 0.7590, val: 0.6230\n",
      "Epoch 24/50, Loss: 0.7730, val: 0.6002\n",
      "Epoch 25/50, Loss: 0.7925, val: 0.6162\n",
      "Epoch 26/50, Loss: 0.7480, val: 0.6198\n",
      "Epoch 27/50, Loss: 0.7392, val: 0.6168\n",
      "Epoch 28/50, Loss: 0.7254, val: 0.6138\n",
      "Epoch 29/50, Loss: 0.7318, val: 0.6222\n",
      "Epoch 30/50, Loss: 0.7260, val: 0.6045\n",
      "Epoch 31/50, Loss: 0.7444, val: 0.6240\n",
      "Epoch 32/50, Loss: 0.7216, val: 0.6250\n",
      "Epoch 33/50, Loss: 0.6936, val: 0.6195\n",
      "Epoch 34/50, Loss: 0.6976, val: 0.6185\n",
      "Epoch 35/50, Loss: 0.6917, val: 0.6205\n",
      "Epoch 36/50, Loss: 0.7027, val: 0.6242\n",
      "Epoch 37/50, Loss: 0.7062, val: 0.6158\n",
      "Epoch 38/50, Loss: 0.7080, val: 0.5988\n",
      "Epoch 39/50, Loss: 0.6927, val: 0.6208\n",
      "Epoch 40/50, Loss: 0.6814, val: 0.6162\n",
      "Epoch 41/50, Loss: 0.6830, val: 0.6218\n",
      "Epoch 42/50, Loss: 0.6698, val: 0.6088\n",
      "Epoch 43/50, Loss: 0.6685, val: 0.6140\n",
      "Epoch 44/50, Loss: 0.6751, val: 0.6098\n",
      "Epoch 45/50, Loss: 0.6884, val: 0.6120\n",
      "Epoch 46/50, Loss: 0.6657, val: 0.6072\n",
      "Epoch 47/50, Loss: 0.6673, val: 0.6118\n",
      "Epoch 48/50, Loss: 0.6793, val: 0.6185\n",
      "Epoch 49/50, Loss: 0.6556, val: 0.6140\n",
      "Epoch 50/50, Loss: 0.6893, val: 0.6175\n",
      "Epoch 1/50, Loss: 1.5437, val: 0.4980\n",
      "Epoch 2/50, Loss: 1.3727, val: 0.5600\n",
      "Epoch 3/50, Loss: 1.2997, val: 0.5827\n",
      "Epoch 4/50, Loss: 1.2336, val: 0.5998\n",
      "Epoch 5/50, Loss: 1.1835, val: 0.5905\n",
      "Epoch 6/50, Loss: 1.1592, val: 0.6035\n",
      "Epoch 7/50, Loss: 1.1146, val: 0.5960\n",
      "Epoch 8/50, Loss: 1.0786, val: 0.6058\n",
      "Epoch 9/50, Loss: 1.0612, val: 0.6210\n",
      "Epoch 10/50, Loss: 1.0194, val: 0.6130\n",
      "Epoch 11/50, Loss: 0.9924, val: 0.6120\n",
      "Epoch 12/50, Loss: 0.9768, val: 0.6165\n",
      "Epoch 13/50, Loss: 0.9417, val: 0.6075\n",
      "Epoch 14/50, Loss: 0.9220, val: 0.6172\n",
      "Epoch 15/50, Loss: 0.9247, val: 0.6170\n",
      "Epoch 16/50, Loss: 0.8964, val: 0.6135\n",
      "Epoch 17/50, Loss: 0.8772, val: 0.6262\n",
      "Epoch 18/50, Loss: 0.8547, val: 0.6092\n",
      "Epoch 19/50, Loss: 0.8396, val: 0.6232\n",
      "Epoch 20/50, Loss: 0.8474, val: 0.6140\n",
      "Epoch 21/50, Loss: 0.8175, val: 0.6175\n",
      "Epoch 22/50, Loss: 0.8049, val: 0.6170\n",
      "Epoch 23/50, Loss: 0.7975, val: 0.6192\n",
      "Epoch 24/50, Loss: 0.8141, val: 0.6085\n",
      "Epoch 25/50, Loss: 0.7731, val: 0.6192\n",
      "Epoch 26/50, Loss: 0.7616, val: 0.6088\n",
      "Epoch 27/50, Loss: 0.7733, val: 0.6162\n",
      "Epoch 28/50, Loss: 0.7758, val: 0.6232\n",
      "Epoch 29/50, Loss: 0.7475, val: 0.6215\n",
      "Epoch 30/50, Loss: 0.7527, val: 0.6110\n",
      "Epoch 31/50, Loss: 0.7423, val: 0.6160\n",
      "Epoch 32/50, Loss: 0.7309, val: 0.6228\n",
      "Epoch 33/50, Loss: 0.7338, val: 0.6128\n",
      "Epoch 34/50, Loss: 0.7214, val: 0.6130\n",
      "Epoch 35/50, Loss: 0.7306, val: 0.6145\n",
      "Epoch 36/50, Loss: 0.7087, val: 0.6165\n",
      "Epoch 37/50, Loss: 0.7130, val: 0.6025\n",
      "Epoch 38/50, Loss: 0.7130, val: 0.6102\n",
      "Epoch 39/50, Loss: 0.7336, val: 0.6065\n",
      "Epoch 40/50, Loss: 0.7238, val: 0.6102\n",
      "Epoch 41/50, Loss: 0.6946, val: 0.6085\n",
      "Epoch 42/50, Loss: 0.7047, val: 0.6002\n",
      "Epoch 43/50, Loss: 0.7095, val: 0.6112\n",
      "Epoch 44/50, Loss: 0.6920, val: 0.6198\n",
      "Epoch 45/50, Loss: 0.6819, val: 0.6145\n",
      "Epoch 46/50, Loss: 0.7005, val: 0.6038\n",
      "Epoch 47/50, Loss: 0.6858, val: 0.6150\n",
      "Epoch 48/50, Loss: 0.6801, val: 0.6098\n",
      "Epoch 49/50, Loss: 0.6812, val: 0.6128\n",
      "Epoch 50/50, Loss: 0.6733, val: 0.6082\n",
      "Epoch 1/50, Loss: 1.5309, val: 0.5212\n",
      "Epoch 2/50, Loss: 1.3575, val: 0.5673\n",
      "Epoch 3/50, Loss: 1.2753, val: 0.5955\n",
      "Epoch 4/50, Loss: 1.2140, val: 0.6035\n",
      "Epoch 5/50, Loss: 1.1697, val: 0.5998\n",
      "Epoch 6/50, Loss: 1.1214, val: 0.6045\n",
      "Epoch 7/50, Loss: 1.0826, val: 0.6165\n",
      "Epoch 8/50, Loss: 1.0691, val: 0.5910\n",
      "Epoch 9/50, Loss: 1.0452, val: 0.6190\n",
      "Epoch 10/50, Loss: 1.0125, val: 0.6208\n",
      "Epoch 11/50, Loss: 0.9818, val: 0.6082\n",
      "Epoch 12/50, Loss: 0.9530, val: 0.6258\n",
      "Epoch 13/50, Loss: 0.9454, val: 0.6085\n",
      "Epoch 14/50, Loss: 0.9176, val: 0.6168\n",
      "Epoch 15/50, Loss: 0.9081, val: 0.6218\n",
      "Epoch 16/50, Loss: 0.8828, val: 0.6165\n",
      "Epoch 17/50, Loss: 0.8823, val: 0.6258\n",
      "Epoch 18/50, Loss: 0.8654, val: 0.6275\n",
      "Epoch 19/50, Loss: 0.8341, val: 0.6232\n",
      "Epoch 20/50, Loss: 0.8231, val: 0.6135\n",
      "Epoch 21/50, Loss: 0.8262, val: 0.6260\n",
      "Epoch 22/50, Loss: 0.8001, val: 0.6208\n",
      "Epoch 23/50, Loss: 0.8046, val: 0.6112\n",
      "Epoch 24/50, Loss: 0.7805, val: 0.6140\n",
      "Epoch 25/50, Loss: 0.7854, val: 0.6182\n",
      "Epoch 26/50, Loss: 0.7696, val: 0.6225\n",
      "Epoch 27/50, Loss: 0.7594, val: 0.6185\n",
      "Epoch 28/50, Loss: 0.7650, val: 0.6148\n",
      "Epoch 29/50, Loss: 0.7412, val: 0.6158\n",
      "Epoch 30/50, Loss: 0.7352, val: 0.6165\n",
      "Epoch 31/50, Loss: 0.7225, val: 0.6180\n",
      "Epoch 32/50, Loss: 0.7249, val: 0.6162\n",
      "Epoch 33/50, Loss: 0.7315, val: 0.6158\n",
      "Epoch 34/50, Loss: 0.7232, val: 0.6208\n",
      "Epoch 35/50, Loss: 0.7525, val: 0.6090\n",
      "Epoch 36/50, Loss: 0.7144, val: 0.6145\n",
      "Epoch 37/50, Loss: 0.7097, val: 0.6120\n",
      "Epoch 38/50, Loss: 0.7024, val: 0.6218\n",
      "Epoch 39/50, Loss: 0.7072, val: 0.6245\n",
      "Epoch 40/50, Loss: 0.7124, val: 0.6085\n",
      "Epoch 41/50, Loss: 0.6988, val: 0.6192\n",
      "Epoch 42/50, Loss: 0.7174, val: 0.6248\n",
      "Epoch 43/50, Loss: 0.7036, val: 0.6140\n",
      "Epoch 44/50, Loss: 0.7104, val: 0.6222\n",
      "Epoch 45/50, Loss: 0.6752, val: 0.6210\n",
      "Epoch 46/50, Loss: 0.6832, val: 0.6300\n",
      "Epoch 47/50, Loss: 0.6687, val: 0.6195\n",
      "Epoch 48/50, Loss: 0.6788, val: 0.6168\n",
      "Epoch 49/50, Loss: 0.6655, val: 0.6170\n",
      "Epoch 50/50, Loss: 0.7021, val: 0.6115\n",
      "Epoch 1/50, Loss: 1.5400, val: 0.5118\n",
      "Epoch 2/50, Loss: 1.3621, val: 0.5777\n",
      "Epoch 3/50, Loss: 1.2707, val: 0.5935\n",
      "Epoch 4/50, Loss: 1.2153, val: 0.6042\n",
      "Epoch 5/50, Loss: 1.1611, val: 0.6140\n",
      "Epoch 6/50, Loss: 1.1208, val: 0.6178\n",
      "Epoch 7/50, Loss: 1.0800, val: 0.6122\n",
      "Epoch 8/50, Loss: 1.0650, val: 0.6165\n",
      "Epoch 9/50, Loss: 1.0344, val: 0.6162\n",
      "Epoch 10/50, Loss: 1.0038, val: 0.6195\n",
      "Epoch 11/50, Loss: 0.9858, val: 0.6198\n",
      "Epoch 12/50, Loss: 0.9600, val: 0.6165\n",
      "Epoch 13/50, Loss: 0.9431, val: 0.6228\n",
      "Epoch 14/50, Loss: 0.9311, val: 0.6152\n",
      "Epoch 15/50, Loss: 0.8941, val: 0.6120\n",
      "Epoch 16/50, Loss: 0.8680, val: 0.6110\n",
      "Epoch 17/50, Loss: 0.8530, val: 0.6165\n",
      "Epoch 18/50, Loss: 0.8512, val: 0.6195\n",
      "Epoch 19/50, Loss: 0.8400, val: 0.6132\n",
      "Epoch 20/50, Loss: 0.8145, val: 0.6150\n",
      "Epoch 21/50, Loss: 0.8030, val: 0.6140\n",
      "Epoch 22/50, Loss: 0.8100, val: 0.6182\n",
      "Epoch 23/50, Loss: 0.7888, val: 0.6082\n",
      "Epoch 24/50, Loss: 0.7880, val: 0.6185\n",
      "Epoch 25/50, Loss: 0.7684, val: 0.6182\n",
      "Epoch 26/50, Loss: 0.7791, val: 0.6140\n",
      "Epoch 27/50, Loss: 0.7628, val: 0.6192\n",
      "Epoch 28/50, Loss: 0.7403, val: 0.6148\n",
      "Epoch 29/50, Loss: 0.7358, val: 0.6192\n",
      "Epoch 30/50, Loss: 0.7324, val: 0.6185\n",
      "Epoch 31/50, Loss: 0.7074, val: 0.6120\n",
      "Epoch 32/50, Loss: 0.7412, val: 0.6128\n",
      "Epoch 33/50, Loss: 0.7184, val: 0.6008\n",
      "Epoch 34/50, Loss: 0.7001, val: 0.6150\n",
      "Epoch 35/50, Loss: 0.7147, val: 0.6162\n",
      "Epoch 36/50, Loss: 0.6985, val: 0.6135\n",
      "Epoch 37/50, Loss: 0.7013, val: 0.6138\n",
      "Epoch 38/50, Loss: 0.7205, val: 0.6200\n",
      "Epoch 39/50, Loss: 0.7231, val: 0.6215\n",
      "Epoch 40/50, Loss: 0.6910, val: 0.6220\n",
      "Epoch 41/50, Loss: 0.6807, val: 0.6118\n",
      "Epoch 42/50, Loss: 0.6676, val: 0.6225\n",
      "Epoch 43/50, Loss: 0.6734, val: 0.6085\n",
      "Epoch 44/50, Loss: 0.6888, val: 0.6140\n",
      "Epoch 45/50, Loss: 0.6881, val: 0.6062\n",
      "Epoch 46/50, Loss: 0.7056, val: 0.6112\n",
      "Epoch 47/50, Loss: 0.7066, val: 0.6072\n",
      "Epoch 48/50, Loss: 0.6857, val: 0.6212\n",
      "Epoch 49/50, Loss: 0.6796, val: 0.6100\n",
      "Epoch 50/50, Loss: 0.6792, val: 0.6222\n",
      "Epoch 1/50, Loss: 1.5319, val: 0.5042\n",
      "Epoch 2/50, Loss: 1.3588, val: 0.5815\n",
      "Epoch 3/50, Loss: 1.2732, val: 0.6020\n",
      "Epoch 4/50, Loss: 1.2081, val: 0.5938\n",
      "Epoch 5/50, Loss: 1.1649, val: 0.6130\n",
      "Epoch 6/50, Loss: 1.1285, val: 0.6120\n",
      "Epoch 7/50, Loss: 1.0991, val: 0.6218\n",
      "Epoch 8/50, Loss: 1.0647, val: 0.6022\n",
      "Epoch 9/50, Loss: 1.0411, val: 0.6045\n",
      "Epoch 10/50, Loss: 1.0110, val: 0.6188\n",
      "Epoch 11/50, Loss: 0.9821, val: 0.6195\n",
      "Epoch 12/50, Loss: 0.9671, val: 0.6185\n",
      "Epoch 13/50, Loss: 0.9301, val: 0.6208\n",
      "Epoch 14/50, Loss: 0.9130, val: 0.6145\n",
      "Epoch 15/50, Loss: 0.8839, val: 0.6212\n",
      "Epoch 16/50, Loss: 0.8820, val: 0.6158\n",
      "Epoch 17/50, Loss: 0.8594, val: 0.6208\n",
      "Epoch 18/50, Loss: 0.8559, val: 0.6112\n",
      "Epoch 19/50, Loss: 0.8200, val: 0.6158\n",
      "Epoch 20/50, Loss: 0.8084, val: 0.6170\n",
      "Epoch 21/50, Loss: 0.8114, val: 0.6140\n",
      "Epoch 22/50, Loss: 0.8172, val: 0.6048\n",
      "Epoch 23/50, Loss: 0.7938, val: 0.6175\n",
      "Epoch 24/50, Loss: 0.7721, val: 0.6272\n",
      "Epoch 25/50, Loss: 0.7839, val: 0.6062\n",
      "Epoch 26/50, Loss: 0.7752, val: 0.6250\n",
      "Epoch 27/50, Loss: 0.7580, val: 0.6305\n",
      "Epoch 28/50, Loss: 0.7517, val: 0.6165\n",
      "Epoch 29/50, Loss: 0.7425, val: 0.6068\n",
      "Epoch 30/50, Loss: 0.7478, val: 0.6155\n",
      "Epoch 31/50, Loss: 0.7211, val: 0.6218\n",
      "Epoch 32/50, Loss: 0.7378, val: 0.6078\n",
      "Epoch 33/50, Loss: 0.7259, val: 0.6002\n",
      "Epoch 34/50, Loss: 0.7219, val: 0.6128\n",
      "Epoch 35/50, Loss: 0.7157, val: 0.6165\n",
      "Epoch 36/50, Loss: 0.7139, val: 0.6162\n",
      "Epoch 37/50, Loss: 0.7124, val: 0.6088\n",
      "Epoch 38/50, Loss: 0.7174, val: 0.6148\n",
      "Epoch 39/50, Loss: 0.6991, val: 0.6140\n",
      "Epoch 40/50, Loss: 0.6874, val: 0.6178\n",
      "Epoch 41/50, Loss: 0.7071, val: 0.6050\n",
      "Epoch 42/50, Loss: 0.7222, val: 0.6082\n",
      "Epoch 43/50, Loss: 0.6967, val: 0.6052\n",
      "Epoch 44/50, Loss: 0.6853, val: 0.6075\n",
      "Epoch 45/50, Loss: 0.6934, val: 0.6085\n",
      "Epoch 46/50, Loss: 0.6794, val: 0.6135\n",
      "Epoch 47/50, Loss: 0.6692, val: 0.6228\n",
      "Epoch 48/50, Loss: 0.6548, val: 0.6065\n",
      "Epoch 49/50, Loss: 0.6735, val: 0.6108\n",
      "Epoch 50/50, Loss: 0.6671, val: 0.6082\n",
      "Epoch 1/50, Loss: 1.5309, val: 0.5092\n",
      "Epoch 2/50, Loss: 1.3498, val: 0.5623\n",
      "Epoch 3/50, Loss: 1.2613, val: 0.6048\n",
      "Epoch 4/50, Loss: 1.1995, val: 0.6060\n",
      "Epoch 5/50, Loss: 1.1509, val: 0.6088\n",
      "Epoch 6/50, Loss: 1.1126, val: 0.6078\n",
      "Epoch 7/50, Loss: 1.0719, val: 0.6088\n",
      "Epoch 8/50, Loss: 1.0422, val: 0.6190\n",
      "Epoch 9/50, Loss: 1.0233, val: 0.6200\n",
      "Epoch 10/50, Loss: 0.9897, val: 0.6178\n",
      "Epoch 11/50, Loss: 0.9477, val: 0.6148\n",
      "Epoch 12/50, Loss: 0.9294, val: 0.6262\n",
      "Epoch 13/50, Loss: 0.9115, val: 0.6275\n",
      "Epoch 14/50, Loss: 0.8795, val: 0.6198\n",
      "Epoch 15/50, Loss: 0.8608, val: 0.5972\n",
      "Epoch 16/50, Loss: 0.8647, val: 0.6228\n",
      "Epoch 17/50, Loss: 0.8566, val: 0.6162\n",
      "Epoch 18/50, Loss: 0.8417, val: 0.6178\n",
      "Epoch 19/50, Loss: 0.8315, val: 0.6162\n",
      "Epoch 20/50, Loss: 0.8082, val: 0.6265\n",
      "Epoch 21/50, Loss: 0.7905, val: 0.6272\n",
      "Epoch 22/50, Loss: 0.7643, val: 0.6082\n",
      "Epoch 23/50, Loss: 0.7820, val: 0.6160\n",
      "Epoch 24/50, Loss: 0.7715, val: 0.6152\n",
      "Epoch 25/50, Loss: 0.7426, val: 0.6125\n",
      "Epoch 26/50, Loss: 0.7462, val: 0.6200\n",
      "Epoch 27/50, Loss: 0.7289, val: 0.6175\n",
      "Epoch 28/50, Loss: 0.7228, val: 0.6142\n",
      "Epoch 29/50, Loss: 0.7386, val: 0.6265\n",
      "Epoch 30/50, Loss: 0.7264, val: 0.6298\n",
      "Epoch 31/50, Loss: 0.7207, val: 0.6080\n",
      "Epoch 32/50, Loss: 0.7250, val: 0.6245\n",
      "Epoch 33/50, Loss: 0.7052, val: 0.6100\n",
      "Epoch 34/50, Loss: 0.6930, val: 0.6260\n",
      "Epoch 35/50, Loss: 0.6950, val: 0.6188\n",
      "Epoch 36/50, Loss: 0.6882, val: 0.6150\n",
      "Epoch 37/50, Loss: 0.6734, val: 0.6202\n",
      "Epoch 38/50, Loss: 0.6933, val: 0.6028\n",
      "Epoch 39/50, Loss: 0.6913, val: 0.6150\n",
      "Epoch 40/50, Loss: 0.6720, val: 0.6208\n",
      "Epoch 41/50, Loss: 0.6717, val: 0.6312\n",
      "Epoch 42/50, Loss: 0.6773, val: 0.6232\n",
      "Epoch 43/50, Loss: 0.6655, val: 0.6040\n",
      "Epoch 44/50, Loss: 0.6704, val: 0.6195\n",
      "Epoch 45/50, Loss: 0.6921, val: 0.6195\n",
      "Epoch 46/50, Loss: 0.6857, val: 0.6155\n",
      "Epoch 47/50, Loss: 0.6798, val: 0.6170\n",
      "Epoch 48/50, Loss: 0.7161, val: 0.6075\n",
      "Epoch 49/50, Loss: 0.6826, val: 0.6280\n",
      "Epoch 50/50, Loss: 0.6603, val: 0.6192\n",
      "Epoch 1/50, Loss: 1.5298, val: 0.5282\n",
      "Epoch 2/50, Loss: 1.3396, val: 0.5807\n",
      "Epoch 3/50, Loss: 1.2518, val: 0.5815\n",
      "Epoch 4/50, Loss: 1.2063, val: 0.5950\n",
      "Epoch 5/50, Loss: 1.1608, val: 0.6075\n",
      "Epoch 6/50, Loss: 1.1220, val: 0.6202\n",
      "Epoch 7/50, Loss: 1.0939, val: 0.6162\n",
      "Epoch 8/50, Loss: 1.0459, val: 0.6198\n",
      "Epoch 9/50, Loss: 1.0257, val: 0.6200\n",
      "Epoch 10/50, Loss: 1.0026, val: 0.6165\n",
      "Epoch 11/50, Loss: 0.9807, val: 0.6145\n",
      "Epoch 12/50, Loss: 0.9438, val: 0.6078\n",
      "Epoch 13/50, Loss: 0.9416, val: 0.6145\n",
      "Epoch 14/50, Loss: 0.9183, val: 0.6198\n",
      "Epoch 15/50, Loss: 0.8836, val: 0.6192\n",
      "Epoch 16/50, Loss: 0.8684, val: 0.6210\n",
      "Epoch 17/50, Loss: 0.8705, val: 0.6172\n",
      "Epoch 18/50, Loss: 0.8543, val: 0.6215\n",
      "Epoch 19/50, Loss: 0.8339, val: 0.6265\n",
      "Epoch 20/50, Loss: 0.8138, val: 0.6168\n",
      "Epoch 21/50, Loss: 0.8173, val: 0.6238\n",
      "Epoch 22/50, Loss: 0.8043, val: 0.6152\n",
      "Epoch 23/50, Loss: 0.7957, val: 0.6125\n",
      "Epoch 24/50, Loss: 0.7942, val: 0.6192\n",
      "Epoch 25/50, Loss: 0.7472, val: 0.6105\n",
      "Epoch 26/50, Loss: 0.7580, val: 0.6210\n",
      "Epoch 27/50, Loss: 0.7439, val: 0.6225\n",
      "Epoch 28/50, Loss: 0.7564, val: 0.6082\n",
      "Epoch 29/50, Loss: 0.7429, val: 0.6142\n",
      "Epoch 30/50, Loss: 0.7150, val: 0.6190\n",
      "Epoch 31/50, Loss: 0.7471, val: 0.6152\n",
      "Epoch 32/50, Loss: 0.7459, val: 0.6112\n",
      "Epoch 33/50, Loss: 0.7207, val: 0.6188\n",
      "Epoch 34/50, Loss: 0.7108, val: 0.6160\n",
      "Epoch 35/50, Loss: 0.7081, val: 0.6225\n",
      "Epoch 36/50, Loss: 0.7040, val: 0.6125\n",
      "Epoch 37/50, Loss: 0.7312, val: 0.6238\n",
      "Epoch 38/50, Loss: 0.7109, val: 0.6198\n",
      "Epoch 39/50, Loss: 0.7009, val: 0.6012\n",
      "Epoch 40/50, Loss: 0.7074, val: 0.6102\n",
      "Epoch 41/50, Loss: 0.7022, val: 0.6115\n",
      "Epoch 42/50, Loss: 0.6771, val: 0.6122\n",
      "Epoch 43/50, Loss: 0.6798, val: 0.6085\n",
      "Epoch 44/50, Loss: 0.6919, val: 0.6138\n",
      "Epoch 45/50, Loss: 0.6735, val: 0.6238\n",
      "Epoch 46/50, Loss: 0.6719, val: 0.6248\n",
      "Epoch 47/50, Loss: 0.6738, val: 0.6255\n",
      "Epoch 48/50, Loss: 0.6864, val: 0.6100\n",
      "Epoch 49/50, Loss: 0.6887, val: 0.6225\n",
      "Epoch 50/50, Loss: 0.6800, val: 0.6172\n",
      "Epoch 1/50, Loss: 1.5124, val: 0.5433\n",
      "Epoch 2/50, Loss: 1.3554, val: 0.5810\n",
      "Epoch 3/50, Loss: 1.2681, val: 0.5745\n",
      "Epoch 4/50, Loss: 1.2042, val: 0.5787\n",
      "Epoch 5/50, Loss: 1.1672, val: 0.6075\n",
      "Epoch 6/50, Loss: 1.1348, val: 0.6060\n",
      "Epoch 7/50, Loss: 1.0896, val: 0.6185\n",
      "Epoch 8/50, Loss: 1.0564, val: 0.6105\n",
      "Epoch 9/50, Loss: 1.0341, val: 0.6085\n",
      "Epoch 10/50, Loss: 1.0011, val: 0.6178\n",
      "Epoch 11/50, Loss: 0.9693, val: 0.6238\n",
      "Epoch 12/50, Loss: 0.9580, val: 0.6122\n",
      "Epoch 13/50, Loss: 0.9388, val: 0.6205\n",
      "Epoch 14/50, Loss: 0.9076, val: 0.6050\n",
      "Epoch 15/50, Loss: 0.8880, val: 0.6135\n",
      "Epoch 16/50, Loss: 0.8994, val: 0.6175\n",
      "Epoch 17/50, Loss: 0.8777, val: 0.6190\n",
      "Epoch 18/50, Loss: 0.8557, val: 0.6190\n",
      "Epoch 19/50, Loss: 0.8331, val: 0.6060\n",
      "Epoch 20/50, Loss: 0.8486, val: 0.6165\n",
      "Epoch 21/50, Loss: 0.8267, val: 0.6160\n",
      "Epoch 22/50, Loss: 0.8097, val: 0.6108\n",
      "Epoch 23/50, Loss: 0.7902, val: 0.6132\n",
      "Epoch 24/50, Loss: 0.7932, val: 0.6162\n",
      "Epoch 25/50, Loss: 0.7821, val: 0.6258\n",
      "Epoch 26/50, Loss: 0.7641, val: 0.6088\n",
      "Epoch 27/50, Loss: 0.7636, val: 0.5972\n",
      "Epoch 28/50, Loss: 0.7799, val: 0.6132\n",
      "Epoch 29/50, Loss: 0.7733, val: 0.6212\n",
      "Epoch 30/50, Loss: 0.7320, val: 0.6168\n",
      "Epoch 31/50, Loss: 0.7307, val: 0.6255\n",
      "Epoch 32/50, Loss: 0.7322, val: 0.6160\n",
      "Epoch 33/50, Loss: 0.7367, val: 0.6162\n",
      "Epoch 34/50, Loss: 0.7201, val: 0.6150\n",
      "Epoch 35/50, Loss: 0.7103, val: 0.6185\n",
      "Epoch 36/50, Loss: 0.7160, val: 0.6090\n",
      "Epoch 37/50, Loss: 0.7382, val: 0.6188\n",
      "Epoch 38/50, Loss: 0.7250, val: 0.6160\n",
      "Epoch 39/50, Loss: 0.7046, val: 0.6130\n",
      "Epoch 40/50, Loss: 0.7204, val: 0.6202\n",
      "Epoch 41/50, Loss: 0.6980, val: 0.6190\n",
      "Epoch 42/50, Loss: 0.6807, val: 0.6090\n",
      "Epoch 43/50, Loss: 0.6866, val: 0.6185\n",
      "Epoch 44/50, Loss: 0.6953, val: 0.6032\n",
      "Epoch 45/50, Loss: 0.6989, val: 0.6108\n",
      "Epoch 46/50, Loss: 0.6786, val: 0.6088\n",
      "Epoch 47/50, Loss: 0.6828, val: 0.6062\n",
      "Epoch 48/50, Loss: 0.6716, val: 0.6235\n",
      "Epoch 49/50, Loss: 0.6795, val: 0.6172\n",
      "Epoch 50/50, Loss: 0.6767, val: 0.6195\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-03T11:33:29.597613Z",
     "start_time": "2024-11-03T11:33:29.578438Z"
    }
   },
   "cell_type": "code",
   "source": "print(result)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         dataset_name  mean_val_accuracy  mean_accuracy  std_accuracy\n",
      "0  [FashionMNIST0, 3]           0.681590       0.939250      0.002624\n",
      "0  [FashionMNIST0, 6]           0.369191       0.818275      0.012256\n",
      "0      [CIFAR10, npz]           0.611625       0.683350      0.006382\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AdvML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
